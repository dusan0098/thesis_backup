# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# # Preprocessing Tweets
# 0. Replace truncated tweets with the complete retweet text
# 1. Remove all URLs
# 2. Replace mentions @some_user with @user if user is not known or full name (from politician metadata) if user is known.
#     2.1. Optionally remove mentions (@username) completely
# 3. Remove hashtags
#     3.1. Optinally - remove them completely VS only the # symbol (can also choose if only those at the end of tweets are affected)
# 4. Remove rows where full_text is "" or NaN
# 6. Replace HTML entities and German umlauts

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# -*- coding: utf-8 -*-
import dataiku
import pandas as pd, numpy as np
from dataiku import pandasutils as pdu
import time
from datetime import datetime
import json
import re
import os
import pickle
import torch

# Read recipe inputs
tweets_raw = dataiku.Dataset("tweets_raw")
df_input = tweets_raw.get_dataframe()

name_mapping = dataiku.Dataset("name_username_mapping")
mapping_df = name_mapping.get_dataframe()

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
specific_types = {}
for column in df_input.columns:
    # Find the first non-null value in the column
    non_null_value = df_input[column].dropna().iloc[0]
    
    # Get the type of this value
    specific_types[column] = type(non_null_value)

print("Column types available in the input data: ",specific_types)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
def count_truncations(df):
    truncated_words = 0
    truncated_hashtags = 0
    truncated_urls = 0
    
    word_truncations = []
    hashtag_truncations = []
    url_truncations = []

    for _, row in df.iterrows():
        full_text = row['full_text']
        if full_text.endswith('...') or full_text.endswith('â€¦'):
            truncated_part = full_text[:-3].strip() if full_text.endswith('...') else full_text[:-1].strip()

            if re.search(r'#\w+$', truncated_part):
                truncated_hashtags += 1
                hashtag_truncations.append(full_text)
            elif re.search(r'https?://\S+$', truncated_part):
                truncated_urls += 1
                url_truncations.append(full_text)
            else:
                truncated_words += 1
                word_truncations.append(full_text)

    total_truncations = truncated_words + truncated_hashtags + truncated_urls

    print(f"Total truncations: {total_truncations}")
    print(f"Words truncated: {truncated_words}")
    print(f"Hashtags truncated: {truncated_hashtags}")
    print(f"URLs truncated: {truncated_urls}")
    print("\nExample of word truncations:")
    for example in word_truncations[:1]:
        print(example)
    print("\nExample of hashtag truncations:")
    for example in hashtag_truncations[:1]:
        print(example)
    print("\nExample of URL truncations:")
    for example in url_truncations[:1]:
        print(example)

# Call the function to count and print truncations
count_truncations(df_input)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
from utils import (
    select_gpu_with_most_free_memory,
    load_experiment_jsons,
    load_experiment_objects,
    get_current_time_and_unix_timestamp,
    get_newest_json,
    get_unique_dictionaries,
    save_combination_list,
)

local_device = select_gpu_with_most_free_memory()
print(f"Using device: {local_device}")

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# # LENGTH DISTRIBUTION BEFORE ANY PREPROCESSING 
# import pandas as pd
# import matplotlib.pyplot as plt

# df = df_input.copy()
# df = df[df["full_text"].notna()]
# # Step 1 & 2: Split text into words and count them
# df['word_count'] = df["full_text"].apply(lambda x: len(re.split(r'\s+', x.strip())))

# # Step 3: Get distribution of word counts
# word_count_distribution = df['word_count'].value_counts().sort_index()

# # Printing the distribution as a table
# print("Word Count Distribution (Table):")
# print(word_count_distribution)

# # Step 4: Plotting the distribution
# plt.figure(figsize=(10, 6))
# word_count_distribution.plot(kind='bar')
# plt.title('Distribution of Word Counts in Text Column')
# plt.xlabel('Number of Words')
# plt.ylabel('Frequency')
# plt.xticks(rotation=45)
# plt.grid(axis='y', linestyle='--', linewidth=0.7)
# plt.show()

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
from event_detection.preprocess_tweets import *

# Example of using preprocessing functions on several columns
#new_df = remove_urls(new_df, ['full_text', 'retweet_full_text'])

def preprocess_tweets(df, mapping_df, config):
    new_df = df.copy(deep=True)
    
    username_to_name = dict(zip(mapping_df['username'], mapping_df['name']))
    
    if config.get["replace_truncated"]:
        new_df = replace_truncated_text(new_df, text_column = "full_text", retweet_column = "retweet_full_text")
        print("Length after replacing truncated tweets:", len(new_df))
    
    if config["remove_urls"]:
        new_df = remove_urls(new_df, ['full_text'])
        print("Length after remove_urls:", len(new_df))
    
    if config["replace_mentions"]:
        function_config = config.get("replace_mentions_config",{})
        new_df = replace_mentions(new_df, ['full_text'], username_to_name, \
                                  replace_fully = function_config.get("replace_fully", False))
        print("Length after replace_mentions:", len(new_df))
        
    if config["remove_hashtags"]:
        function_config = config.get("remove_hashtags_config",{})
        new_df = remove_hashtags(new_df, ['full_text'], on_end_only = function_config.get("on_end_only", False),\
                                 remove_fully = function_config.get("remove_fully", False))
        print("Length after remove_hashtags:", len(new_df))
        
    if config["remove_empty_rows"]:
        new_df = remove_empty_rows(new_df, ['full_text'])
        print("Length after remove_empty_rows:", len(new_df))
        
    if config["remove_short_tweets"]:
        function_config = config["remove_short_tweets_config"]
        new_df = remove_short_tweets(new_df, ['full_text'], min_word_count = function_config.get("min_word_count",5))
        print("Length after remove_short_tweets:", len(new_df))
        
    if config.get("replace_html_entities", False):
        new_df = replace_html_entities(new_df, ['full_text'])
        print("Length after replacing HTML entities:", len(new_df))
    
    if config.get("replace_german_umlauts", False):
        new_df = replace_german_umlauts(new_df, ['full_text'])
        print("Length after replacing German umlauts:", len(new_df))
    
    return new_df

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
def preprocess_and_save(df_input, mapping_df, dataset_name, config, root_folder_path):
    # Current UNIX timestamp for unique file names and readable timestamps
    unix_timestamp, readable_timestamp = get_current_time_and_unix_timestamp()

    # Applying preprocessing steps
    processed_df = preprocess_tweets(df_input, mapping_df, config)
    
    columns_to_drop = ['retweeted_status', 'retweet_full_text', 'available', 'in_reply_to_user_id_str', \
                       'followers_count', 'location']
    
    processed_df = processed_df.drop(columns=columns_to_drop, axis=1, errors='ignore')

    # Subfolder paths within the Dataiku managed folder
    dataset_subfolder = os.path.join(root_folder_path, "dataset")
    os.makedirs(dataset_subfolder, exist_ok=True)
    
    preprocessing_steps_subfolder = os.path.join(root_folder_path, "preprocessing_steps")
    os.makedirs(preprocessing_steps_subfolder, exist_ok=True)

    # File names and paths
    dataset_file_name = f"{dataset_name}_{unix_timestamp}.pkl"
    preprocessing_details_file_name = f"{dataset_name}_details_{unix_timestamp}.json"
    
    dataset_path = os.path.join(dataset_subfolder, dataset_file_name)
    preprocessing_details_path = os.path.join(preprocessing_steps_subfolder, preprocessing_details_file_name)

    # Save the preprocessed DataFrame in pickle format
    with open(dataset_path, 'wb') as f:  # Open the file in binary write mode
        pickle.dump(processed_df, f)

    # Preprocessing details including dataset name, columns, and an example row
    preprocessing_details = {
        "dataset_name": dataset_name,
        "timestamp": readable_timestamp, 
        "dataset_location": dataset_path,
        "columns": list(processed_df.columns),
        "example_row": json.loads(processed_df.iloc[0].to_json()),
        "preprocessing_steps": config,
    }

    # Save preprocessing details as JSON
    with open(preprocessing_details_path, 'w') as json_file:
        json.dump(preprocessing_details, json_file, indent=4)

    print(f"Preprocessed data and details saved successfully for {dataset_name}.")
    return preprocessing_details, processed_df

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# ### Example: Loading all JSONs for experiments that were performed

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
folder = dataiku.Folder("PpRdk4F7")
folder_path = folder.get_path()

experiment_jsons=load_experiment_jsons(
                            root_folder_path = folder_path,
                            dataset_name = "",
                            experiment_details_subfolder = "preprocessing_steps")
print(experiment_jsons)
print(f"Number of experiments found: {len(experiment_jsons)}")
if len(experiment_jsons)>0:
    print("Example experiment:", json.dumps(experiment_jsons[0], indent = 4))

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# ### Example: Getting the newest JSON from a list based on a timestamp field

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# Get the newest JSON based on the timestamp
newest_json = get_newest_json(experiment_jsons, timestamp_key="timestamp")
if newest_json:
    print("Newest JSON:", newest_json)
else:
    print("No newest JSON found.")

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# ### Example: Loading all objects from JSONs based on file_path field

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# experiment_objects = load_experiment_objects(experiment_jsons = experiment_jsons, 
#                                              file_path_key = "dataset_location")

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
#type(experiment_objects[0])

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
get_current_time_and_unix_timestamp()

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# # Setting up the combinations

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
preprocessing_combinations = [
{
    "replace_truncated": True,
    "remove_urls": True,
    "replace_mentions": True,
    "replace_mentions_config":{
        # If true remove whole mentions otherwise just substitutes to @user/Name of person
        "replace_fully": True,
     },
    "remove_hashtags": True,
    "remove_hashtags_config":{
        "on_end_only": False,
       "remove_fully": False,
    },
    "remove_empty_rows": True,
    "remove_short_tweets": True,
    "remove_short_tweets_config":{
        "min_word_count":4
    },
    "replace_html_entities": True,
    "replace_german_umlauts": False,
},
    # DEFINE ALTERNATIVE PREPROCESS
# {
#     "replace_truncated": True,
#     "remove_urls": True,
#     "replace_mentions": True,
#     "remove_hashtags": True,
#     "remove_hashtags_config":{
#         "on_end_only": False,
#        "remove_fully": False,
#     },
#     "remove_empty_rows": True,
#     "remove_short_tweets": True,
#     "remove_short_tweets_config":{
#         "min_word_count":5
#     },
# },
]

# Dataiku managed folder access
output_folder = dataiku.Folder("PpRdk4F7")
output_folder_path = output_folder.get_path()
dataset_name = "german_tweets"

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# ### Example: Getting unique dictionaries (combinations) from a list of dicts

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# Get unique combinations for preprocessing
candidate_configs = get_unique_dictionaries(preprocessing_combinations, 
                                            return_strings = False)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
processed_configs = [j["preprocessing_steps"] for j in experiment_jsons]

new_configs = []
for config in candidate_configs:
    if config in processed_configs:
        print("Config already performed:", config)
    else:
        new_configs.append(config)
        
print(f"There are {len(new_configs)} new configs:")
print(new_configs)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# # Running preprocess over all new combinations

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
new_experiments = []
for curr_config in new_configs:
    print("Running preprocessing for following config:", curr_config)
    curr_experiment, processed_df = preprocess_and_save(df_input,
                        mapping_df, 
                        dataset_name,
                        curr_config, 
                        root_folder_path = output_folder_path)
    print(processed_df.head(1))
    new_experiments.append(curr_experiment)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# ### Example: Saving processed_combinations
# - **Note** - make sure to add the new_configs to the processed_configs extracted from the old JSONs

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
experiment_jsons.extend(new_experiments)
save_combination_list(combination_list = experiment_jsons, 
                       root_folder_path = output_folder_path)