# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# -*- coding: utf-8 -*-
import dataiku
import pandas as pd, numpy as np
from dataiku import pandasutils as pdu
import os 
import openai
import json
import ast
import time
from tqdm import tqdm

# Recipe inputs
ed_clusters_with_reps = dataiku.Folder("oiEvOl5p")
ed_clusters_with_reps_path = ed_clusters_with_reps.get_path()

# Defining file with clusters
file_name = "clustering_sentence-transformers-paraphrase-multilingual-MiniLM-L12-v2_min_cluster_size10_min_samples10_alpha1.0_cluster_selection_methodleaf_20240614_105503_UTC.pkl"
pickle_file_path = os.path.join(ed_clusters_with_reps_path, "clustering_results",file_name)

tweets_df = pd.read_pickle(pickle_file_path)
tweets_df.rename(columns={'repr_docs': 'representative_docs'}, inplace=True)

# Recipe outputs
daily_event_extraction = dataiku.Dataset("daily_event_extraction")

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
from openai_utils import handle_rate_limit_error, handle_json_format_error
# OpenAI configuration
openai.api_key = "a796cd0d45604c42b9738d7900c11861"
openai.api_base = "https://topic-representation-long.openai.azure.com/"
openai.api_type = "azure"
openai.api_version = '2023-05-15'
deployment_name = 'ChatGPT-Long'

openai_config = {
    "max_tokens": 8000,
    "temperature": 0.2,
    "top_p": 0.2
}

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# ### Filtering based on date

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
import datetime
start_date = datetime.date(2019, 8, 1)
end_date = datetime.date(2019, 12, 1)
tweets_df = tweets_df[(tweets_df["date"]>=start_date) \
                     &(tweets_df["date"]<=end_date)]

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
system_message = {
    "role": "system",
    "content": "You are a helpful assistant. Your primary task is to determine whether the provided cluster of tweets, \
    extracted from German politicians' Twitter accounts, represents an important event or a regular (everyday) conversation.\
    The analysis should consider the features and representative documents (tweets) provided for a specific period. \
    If the content represents an important event, the event should be tied to something exclusive to the period and \
    not a topic that would be discussed by the politicians on a regular basis. The response should always be in English.\
    Based on the analysis, please provide a response in a structured JSON format with the following fields: \
    - 'is_event': Boolean (True if the cluster represents an event, False otherwise) \
    - 'event_name': A short title for the event (in English, use German words only if necessary) \
    - 'event_description': A brief explanation of the event in 1-2 sentences (in English, focusing on who, what, where, when, and why) \
    If the content does not represent an event, set 'is_event' to False and provide default values for 'event_name' and 'event_description'."
}

def create_prompt(row):
    date = row['date']
    ctfidf_representation = ', '.join(row['ctfidf_representation'])
    keybert_representation = ', '.join(row['keybert_representation'])
    representative_docs = '\n'.join(row['representative_docs'])

    event_examples = """
    An event should be something significant that had a notable effect, reaction, controversy, or discourse. Here are examples of what should be considered an event:
    - A controversial law that sparked significant debate or protests
    - Fights or significant altercations in the parliament
    - Changes in leadership within the parliament or political parties
    - Wars or military actions
    - Natural disasters
    - Large protests or public demonstrations
    - Major scandals or political controversies

    Conversely, a regular conversation should include routine activities or discussions that do not lead to significant reactions or impact. For example:
    - Routine readings of laws that were expected and had no significant impact
    - Standard political meetings without notable outcomes
    - General discussions about ongoing policies without new developments
    """

    prompt = f"""
    I am providing you with the representative features and documents of a cluster extracted from tweets posted by \
    German politicians. The cluster was created using tweets scraped from their official Twitter accounts, and the\
    content includes both German and English languages. The day on which these tweets were posted is on {date}.

    Please analyze the provided features and representative documents to determine whether the content represents\
    a regular (everyday) conversation or an important event being discussed. An event should be tied to something \
    exclusive to that day and not a topic that would be discussed by the politicians on a regular basis. The event title \
    should precisely capture what is happening on that day and why these tweets were posted. \
    Here are the details:

    ### Date:
    - Date: {date}

    ### Representative Features:
    - C-TF-IDF Representation: {ctfidf_representation}
    - KeyBERT Representation: {keybert_representation}

    ### Representative Documents (Tweets):
    {representative_docs}

    {event_examples}

    Based on this information, is the cluster discussing a regular conversation or an important event? \
    Please provide your reasoning in the following JSON format:

    {{
        "is_event": true or false,
        "event_name": "string",
        "event_description": "string"
    }}.

    If you determine that the cluster corresponds to an event (is_event is true), please fill out all of the fields with \
    the following details:
    - "is_event": true
    - "event_name": A short title for the event (in English, use German words only if necessary)
    - "event_description": An explanation of the event with key details. \
    (in English, focusing on conveying - who, what, where, when, and why. Ensure that the event description explains \
    the core reason and consequences of the event happening on that day.) If the event is part of a larger ongoing event\
    you can also provide this context through an additional sentence, after covering the daily event.

    If you determine that the cluster does not correspond to an event (is_event is false), please fill out the fields with:
    - "is_event": false
    - "event_name": ""
    - "event_description": ""
    """
    return prompt

# Example usage with a DataFrame row
example_row = tweets_df.iloc[0]
example_prompt = create_prompt(example_row)
#example_prompt

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# Function to process each row and get the GPT-3 response with error handling
def process_row(row):
    current_prompt = create_prompt(row)
    messages = [system_message, {"role": "user", "content": current_prompt}]

    max_attempts = 3  # Maximum attempts to get correct JSON
    for attempt in range(max_attempts):
        try:
            response = openai.ChatCompletion.create(
                engine=deployment_name,
                messages=messages,
                **openai_config
            )
            content = response['choices'][0]['message']['content']
            result = json.loads(content)
            return result
        except openai.error.InvalidRequestError as e:
            if "maximum context length" in str(e):
                print(f"Context too long in attempt {attempt + 1}.")
            else:
                print(f"Attempt {attempt + 1} failed. Error: {str(e)}")
                break  # Exit the loop for invalid request errors
        except json.decoder.JSONDecodeError as e:
            result = handle_json_format_error(
                current_messages=messages,
                deployment_name=deployment_name,
                response=response,
                error_message=e,
                max_attempts=max_attempts
            )
            return result
        except openai.error.RateLimitError as e:
            print(f"RateLimitError in attempt {attempt + 1}")
            if attempt < max_attempts - 1:
                error_message = str(e)
                handle_rate_limit_error(error_message)
            else:
                print("Maximum retry attempts reached.")
        except Exception as e:
            print(f"Error in attempt {attempt + 1}: {str(e)}")
            if attempt == max_attempts - 1:
                handle_rate_limit_error(str(e))
            else:
                time.sleep(5)  # Wait for 5 seconds before retrying
    return {"is_event": False, "event_name": "", "event_description": ""}

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# Adding new columns to tweets_df for the results
tweets_df['is_event'] = False
tweets_df['event_name'] = ""
tweets_df['event_description'] = ""

# Save frequency
save_freq = 100
daily_event_extraction = dataiku.Dataset("daily_event_extraction")

# Iterate over each row in tweets_df and process it
for idx, row in tqdm(tweets_df.iterrows(), desc="Processing topic clusters"):
    if (idx % save_freq == 0) and (idx != 0):
        print(f"Saving dataframe for index: {idx}")
        # Save results periodically
        daily_event_extraction.write_with_schema(tweets_df)

    result = process_row(row)
    tweets_df.at[idx, 'is_event'] = result.get('is_event', False)
    if result['is_event']:
        print("\nEvent name: ", result.get('event_name', "")) 
        print("\nEvent description: ", result.get('event_description', ""))
        tweets_df.at[idx, 'event_name'] = result.get('event_name', "")
        tweets_df.at[idx, 'event_description'] = result.get('event_description', "")
    else:
        print('not an event')

# Final save of results
daily_event_extraction.write_with_schema(tweets_df)