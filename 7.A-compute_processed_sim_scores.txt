# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# ### SIMILARITY IDEAS
# 1. Try to extract KeyBERT scores and do weighted sim
# 2. Use sim with "" as baseline 
# 3. Use max_sim of topics in the same period as threshold
# 4. Forbenius/Nuclear norm - L1/L2 for matrix - Use it as scaling factor prior to Clustering (HDBSCAN)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# -*- coding: utf-8 -*-
import dataiku
import pandas as pd, numpy as np
from dataiku import pandasutils as pdu
from datetime import datetime
from utils import (
    select_gpu_with_most_free_memory,
    load_experiment_jsons,
    load_experiment_objects,
    get_current_time_and_unix_timestamp,
    get_newest_json,
    get_unique_dictionaries,
    save_combination_list,
)

# Read recipe inputs
ed_similarity_scores = dataiku.Folder("0JFzvl2d")
ed_similarity_scores_path = ed_similarity_scores.get_path()

# Write recipe outputs
ed_similarity_scores_processed = dataiku.Folder("hZfSC2LV")
ed_similarity_scores_processed_info = ed_similarity_scores_processed.get_path()

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
experiment_jsons = load_experiment_jsons(
                            root_folder_path = ed_similarity_scores_path,
                            dataset_name = "",
                            experiment_details_subfolder = "similarity_experiment_details")

# Add filters if necessary
filtered_jsons = [experiment_jsons[0]]
#filtered_jsons

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
device = select_gpu_with_most_free_memory()

for curr_json in filtered_jsons:
    similarity_scores = load_experiment_objects(experiment_jsons = [curr_json],
                            file_path_key = "similarity_save_location")[0]
    
    clusters_df = load_experiment_objects(experiment_jsons = [curr_json],
                            file_path_key = "clustering_save_location")[0]
    
    # Ensure 'clusters_df' has the correct column order
    primary_columns = ['topic_id', 'date', 'cluster_size']
    secondary_columns = [col for col in clusters_df.columns if col not in primary_columns]
    clusters_df = clusters_df[primary_columns + secondary_columns]

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
#pd.set_option('display.max_colwidth',3000)
clusters_df.tail(3)[["ctfidf_representation","keybert_representation"]]#.values

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
print(len(similarity_scores))
print("Unique Clusters * days:", 8073 * 480)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
#similarity_scores= similarity_scores[similarity_scores["timeline_date"]!=pd.Timestamp("2019-01-02")]

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
similarity_scores.head(3)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# ### Imputing values for missing dates

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
df = similarity_scores.copy()
sim_columns = [col for col in df.columns if col.startswith('max_sim_')]

# Sorting values
df.sort_values(['date', 'topic_id', 'timeline_date'], inplace=True)

# Determine full timeline
full_timeline = pd.date_range(start=df['timeline_date'].min(), end=df['timeline_date'].max(), freq='D')

# Create a multi-index with only existing combinations of topic_id, date, and the full timeline
all_combinations = pd.MultiIndex.from_tuples(
    [(topic_id, date, timeline_date) for (topic_id, date) in df[['topic_id', 'date']].drop_duplicates().itertuples(index=False)
     for timeline_date in full_timeline],
    names=['topic_id', 'date', 'timeline_date']
)

# Reindex the DataFrame to ensure all combinations are present
df.set_index(['topic_id', 'date', 'timeline_date'], inplace=True)
df = df.reindex(all_combinations)

# Interpolate missing values within each (topic_id, date) combination, ensuring correct order
def interpolate_group(group):
    group = group.sort_index()  # Ensure sorting by timeline_date within each group
    group[sim_columns] = group[sim_columns].interpolate(method='linear')
    return group

df = df.groupby(['topic_id', 'date']).apply(interpolate_group)

# Reset index to prepare for pivot
df.reset_index(inplace=True)

# Transform to wide format: create lists of scores for each (topic_id, date) combination
def aggregate_scores(group):
    return pd.Series({f'{col}_list': group[col].tolist() for col in sim_columns})

# Apply the aggregation
df_wide = df.groupby(['topic_id', 'date']).apply(aggregate_scores).reset_index()

# Sort in order of topics
df_wide.sort_values(['date', 'topic_id'], inplace=True)

# Display the wide DataFrame
pd.reset_option('display.max_colwidth') 
df_wide.head(3)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
#len(df_wide.iloc[0]["max_sim_keybert_list"])
timeline_columns = [col for col in df_wide.columns if col.endswith('_list')]
for column in timeline_columns:
    print("Column name:", column)
    print(df_wide[column].str.len().value_counts())

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
df_wide = df_wide.merge(clusters_df, on=["topic_id","date"])

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
df_wide.head(3)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
import pandas as pd
import numpy as np
from collections import Counter

def find_lowest_dates(aggregated_df, sim_column="max_sim_keybert_representation_average_list"):
    # Extract the global start and end dates from the DataFrame
    global_start_date = aggregated_df['date'].min()
    global_end_date = aggregated_df['date'].max()

    # Generate a timeline from the global start to end date
    timeline = pd.date_range(start=global_start_date, end=global_end_date)

    # Dictionary to store the counts of dates appearing in the lowest 10 values
    date_counts = Counter()

    # Iterate through each row in the DataFrame
    for _, row in aggregated_df.iterrows():
        # Convert Max_Similarities list to a Series with a proper datetime index
        cluster_timeline = pd.Series(data=row[sim_column],
                                     index=pd.date_range(start=global_start_date,
                                                         end=global_end_date))

        # Reindex to the global timeline, filling missing days with NaN, then interpolate
        cluster_timeline = cluster_timeline.reindex(timeline, method='pad')

        # Find the dates of the 10 lowest values
        lowest_dates = cluster_timeline.nsmallest(10).index

        # Update the counts for these dates
        date_counts.update(lowest_dates)

    # Convert the Counter to a DataFrame for easier sorting and display
    date_counts_df = pd.DataFrame.from_dict(date_counts, orient='index', columns=['count'])
    date_counts_df.index.name = 'date'
    date_counts_df.reset_index(inplace=True)

    # Sort by count in descending order for better legibility
    date_counts_df.sort_values(by='count', ascending=False, inplace=True)

    return date_counts_df

# Usage example
lowest_dates_df = find_lowest_dates(aggregated_df  = df_wide, sim_column="max_sim_keybert_representation_average_list")

# Display the sorted DataFrame
lowest_dates_df

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
clusters_df[clusters_df["date"]==datetime.date(2019,4,18)].ctfidf_representation.values

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
from statsmodels.nonparametric.smoothers_lowess import lowess
import datetime

def visualize_loess_smoothing(aggregated_df, N, alpha, sim_column = "max_sim_keybert_list"):    
    aLL_scores = np.array(np.concatenate(aggregated_df[sim_column]))
    q_lower = np.percentile(aLL_scores, 70) # higher makes it easier
    q_upper = np.percentile(aLL_scores, 40) # lower makes it easier

    # Extract the global start and end dates from the DataFrame
    global_start_date = aggregated_df['date'].min()
    global_end_date = aggregated_df['date'].max()

    # Generate a timeline from the global start to end date
    timeline = pd.date_range(start=global_start_date, end=global_end_date)
    
    # TESTING FILTER
    filter_date = datetime.date(2020, 2, 5)
    aggregated_df = aggregated_df[aggregated_df["date"] == filter_date]
    
    # Sample N random clusters
    sampled_clusters = aggregated_df.sample(n=N,replace=False)

    # Plotting
    fig, axs = plt.subplots(N, 1, figsize=(10, N * 4))

    if N == 1:
        axs = [axs]  # Make sure axs is iterable for a single plot

    for ax, (_, row) in zip(axs, sampled_clusters.iterrows()):
        # Convert Max_Similarities list to a Series with a proper datetime index
        cluster_timeline = pd.Series(data=row[sim_column],
                                     index=pd.date_range(start=global_start_date,
                                                         end=global_end_date))

        # Reindex to the global timeline, filling missing days with NaN, then interpolate
        cluster_timeline = cluster_timeline.reindex(timeline, method='pad')

        # Apply LOESS smoothing
        smoothed_values = lowess(cluster_timeline, np.arange(len(cluster_timeline)), frac=alpha)[:, 1]

        # Plot original and smoothed values
        ax.plot(timeline, cluster_timeline, label='Original', alpha=0.4)
        ax.plot(timeline, smoothed_values, label='LOESS Smoothed', color='red')
        ax.hlines(y=q_lower, xmin = timeline.min(),xmax = timeline.max(),linewidth=1, color='black')
        ax.hlines(y=q_upper, xmin = timeline.min(),xmax = timeline.max(),linewidth=1, color='black')
        ax.vlines(x = filter_date, ymin =smoothed_values.min(), ymax = smoothed_values.max(), linewidth=1, color='green')

        ax.set_title(f'{row["date"]}-{row["ctfidf_representation"][:6]}')
        ax.set_xlabel('Observation Date')
        ax.set_ylabel('Max Similarity')
        ax.legend()

    plt.tight_layout()
    plt.show()

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
sim_column = "max_sim_keybert_representation_average_list"
aLL_scores = np.array(np.concatenate(df_wide[sim_column]))
q_lower = np.percentile(aLL_scores, 70)
q_upper = np.percentile(aLL_scores, 40)
print(q_lower,q_upper)


df_test = df_wide.copy()
visualize_loess_smoothing(df_test, N=5, alpha=0.04, sim_column = sim_column)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# ### VISUALISATIONS FOR THESIS

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# Visualising the disritubtion of similarity scores
import seaborn as sns
import matplotlib.pyplot as plt

def plot_global_distribution(data, column_name):
    """
    Plot the global distribution of a column's values.
    :param data: DataFrame containing the data.
    :param column_name: Name of the column to plot.
    """
    plt.figure(figsize=(10, 6))
    sns.histplot(data[column_name], kde=True)
    plt.title(f'Distribution of {column_name}')
    plt.xlabel(column_name)
    plt.ylabel('Frequency')
    plt.show()

# Example usage
#sim_column = "max_sim_keybert_representation_average_list"
#plot_global_distribution(similarity_scores, sim_column)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# Visualising the disritubtion of similarity scores if we perform Clipping with quantiles
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

def enhanced_robust_scale_scores(scores):
    """
    Scale the scores robustly using quantiles for the lower bound and fix the upper bound to 1.
    :param scores: A list or array of similarity scores.
    :return: Scaled scores as a numpy array.
    """
    q1 = np.percentile(scores, 3)#1   # 1st percentile for lower bound
    max_value = 1.0  # Fix the upper bound to 1

    # Clip scores to quantile range for lower bound only
    scores_clipped = np.clip(scores, q1, max_value)
    
    # Scale scores to 0-1 range
    scaled_scores = (scores_clipped - q1) / (max_value - q1)
    
    return scaled_scores

# Example usage
scores = np.array(similarity_scores["max_sim_keybert_representation_average"])  # Example similarity scores
#scaled_scores = enhanced_robust_scale_scores(scores)

# Plotting the scaled scores to visualize the distribution
# plt.figure(figsize=(10, 6))
# sns.histplot(scaled_scores, kde=True)
# plt.title('Distribution of Scaled Similarity Scores')
# plt.xlabel('Scaled Similarity Score')
# plt.ylabel('Frequency')
# plt.show()

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# ### OLD CODE

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# OLD CODE - HAD TO DROP UNNECESSARY COMBINATIONS
# df = similarity_scores.copy()
# # Convert date columns to datetime
# df['date'] = pd.to_datetime(df['date'])
# df['timeline_date'] = pd.to_datetime(df['timeline_date'])
# df.sort_values(['date', 'topic_id','timeline_date'],inplace = True)

# # Determine full timeline
# full_timeline = pd.date_range(start=df['timeline_date'].min(), end=df['timeline_date'].max(), freq='D')

# combinations = df[['topic_id', 'date']].value_counts().index
# # Create a multi-index with all combinations of topic_id, date, and the full timeline
# all_combinations = pd.MultiIndex.from_product(
#     [df['topic_id'].unique(), df['date'].unique(), full_timeline],
#      names=['topic_id', 'date', 'timeline_date'],
# )

# # Reindex the DataFrame to ensure all combinations are present
# df.set_index(['topic_id', 'date', 'timeline_date'], inplace=True)
# df = df.reindex(all_combinations)

# # Interpolate missing values within each (topic_id, date) combination
# df = df.groupby(['topic_id', 'date']).apply(lambda group: group.interpolate(method='linear'))

# # Removes all combinations that didn't exist in original set
# df = df.dropna(subset=['max_sim_ctfidf','max_sim_keybert'])

# # Reset index to prepare for pivot
# df.reset_index(inplace=True)

# # Transform to wide format: create lists of scores for each (topic_id, date) combination
# def aggregate_scores(group):
#     return pd.Series({
#         'max_sim_ctfidf_list': group['max_sim_ctfidf'].tolist(),
#         'max_sim_keybert_list': group['max_sim_keybert'].tolist(),
#     })

# # Apply the aggregation
# df_wide = df.groupby(['topic_id', 'date']).apply(aggregate_scores).reset_index()

# # Sort in order of topics
# df_wide.sort_values(['date', 'topic_id'],inplace = True)

# # Display the wide DataFrame
# df_wide.head()