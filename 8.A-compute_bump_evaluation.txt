# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# -*- coding: utf-8 -*-
import dataiku
import pandas as pd, numpy as np
from dataiku import pandasutils as pdu
import matplotlib.pyplot as plt
import seaborn as sns
import random
import datetime
from utils import (
    select_gpu_with_most_free_memory,
    load_experiment_jsons,
    load_experiment_objects,
    get_current_time_and_unix_timestamp,
    get_newest_json,
    get_unique_dictionaries,
    save_combination_list,
)
import ast
from tqdm import tqdm

# Input folders
ed_clusters_with_reps = dataiku.Folder("oiEvOl5p")
ed_clusters_with_reps_path = ed_clusters_with_reps.get_path()

ed_similarity_scores = dataiku.Folder("0JFzvl2d")
ed_similarity_scores_path= ed_similarity_scores.get_path()

ed_similarity_scores_processed = dataiku.Folder("hZfSC2LV")
ed_similarity_scores_processed_path = ed_similarity_scores_processed.get_path()

# Input datasets
weekly_events_merged = dataiku.Dataset("weekly_events_merged")
weekly_events_df = weekly_events_merged.get_dataframe()

key_events_per_period = dataiku.Dataset("key_events_per_period")
key_events_df = key_events_per_period.get_dataframe()

# Fixing lists
weekly_events_df['representative_docs'] = weekly_events_df['representative_docs'].apply(ast.literal_eval)
weekly_events_df['ctfidf_representation'] = weekly_events_df['ctfidf_representation'].apply(ast.literal_eval)
weekly_events_df['keybert_representation'] = weekly_events_df['keybert_representation'].apply(ast.literal_eval)
weekly_events_df['tweet_ids'] = weekly_events_df['tweet_ids'].apply(ast.literal_eval)

# Fixing date columns to get rid of hours, mins, seconds
weekly_events_df['start_date'] = pd.to_datetime(weekly_events_df['start_date']).dt.date
weekly_events_df['end_date'] = pd.to_datetime(weekly_events_df['end_date']).dt.date

# Fixing date columns to get rid of hours, mins, seconds
key_events_df['start_date'] = pd.to_datetime(key_events_df['start_date']).dt.date
key_events_df['end_date'] = pd.to_datetime(key_events_df['end_date']).dt.date

key_events_df.rename(columns={"key_event_id": "topic_id"},inplace = True)
key_events_df = key_events_df.merge(weekly_events_df, on=['topic_id', 'start_date'],suffixes=('', '_y'))
key_events_df.drop(key_events_df.filter(regex='_y$').columns, axis=1, inplace=True)

# Output folder
bump_evaluation = dataiku.Folder("bFPFsUEv")
bump_evaluation_path = bump_evaluation.get_path()

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
print("Number of key ground truth events: ",len(key_events_df))
key_events_df.head(1)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
experiment_jsons = load_experiment_jsons(
                            root_folder_path = ed_similarity_scores_processed_path,
                            dataset_name = "",
                            experiment_details_subfolder = "bump_detection_experiment_details")
# Add filter if necessary
filtered_jsons = experiment_jsons
# filtered_jsons

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# ### LOADING ALL STRUCTURES FOR CURRENT EXPERIMENT

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
for curr_json in filtered_jsons:
    clusters_df  = load_experiment_objects(experiment_jsons = [curr_json],
                            file_path_key = "clustering_save_location")[0]
    
    # Ensure 'clusters_df' has the correct column order
    primary_columns = ['topic_id', 'date', 'cluster_size']
    secondary_columns = [col for col in clusters_df.columns if col not in primary_columns]
    clusters_df = clusters_df[primary_columns + secondary_columns]
    
#     similarity_scores = load_experiment_objects(experiment_jsons = [curr_json],
#                             file_path_key = "similarity_save_location")[0]
    
    bump_df  = load_experiment_objects(experiment_jsons = [curr_json],
                            file_path_key = "bump_save_location")[0]
    
    bump_df = bump_df.merge(clusters_df, on=["topic_id","date"])
    event_candidates_df = bump_df[bump_df["has_bumps"]]

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# ### Extracting ground_truth events

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
max_delta_days = 7

# Create a condition to filter rows - has bump and bump is close
condition = (bump_df['closest_bump_peak_date'].isna()) \
            | (abs((bump_df['closest_bump_peak_date'] - bump_df['date']).apply(lambda x: x.days)) <= max_delta_days) 

# Filter the DataFrame based on the condition
close_bumps_df = bump_df[condition]

# Display the filtered DataFrame
close_bumps_df

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# 1. Counting the number of rows (configs) where has_bumps is True for each cluster (grouped by date and topic_id)
cluster_bump_counts = close_bumps_df.groupby(['date', 'topic_id'])['has_bumps'].sum().reset_index()
cluster_bump_counts.rename(columns={'has_bumps': 'num_bumps'}, inplace=True)

# 2. Counting the number of unique similarity_column values where at least one config has has_bumps as True
# First, create a boolean DataFrame to indicate if there's any bump for each similarity_column within each cluster
bump_presence_per_similarity = close_bumps_df.groupby(['date', 'topic_id', 'similarity_column'])['has_bumps'].any().reset_index()

# Then, count the unique similarity_column values where has_bumps is True for each cluster
similarity_bump_counts = bump_presence_per_similarity.groupby(['date', 'topic_id'])['has_bumps'].sum().reset_index()
similarity_bump_counts.rename(columns={'has_bumps': 'num_similarity_columns_with_bumps'}, inplace=True)

# Ensuring all clusters are included by merging with the original clusters
all_clusters = close_bumps_df[['date', 'topic_id']].drop_duplicates()
similarity_bump_counts = all_clusters.merge(similarity_bump_counts, on=['date', 'topic_id'], how='left').fillna(0)
similarity_bump_counts['num_similarity_columns_with_bumps'] = similarity_bump_counts['num_similarity_columns_with_bumps'].astype(int)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
#cluster_bump_counts.sort_values(by = "num_bumps", ascending = False)
#similarity_bump_counts.sort_values(by = "num_similarity_columns_with_bumps", ascending = False)

# Plotting Histogram and Density for Config Counts
plt.figure(figsize=(12, 6))
sns.histplot(cluster_bump_counts['num_bumps'], kde=False, bins=range(0, 34))
plt.title('Histogram and Density for Config Counts')
plt.xlabel('Config number')
plt.ylabel('Frequency')
plt.xticks(range(0, 34, 2))  # Adjust x-axis ticks to show every second number
plt.show()

# Plotting Histogram and Density for Similarity Column Counts
plt.figure(figsize=(12, 6))
sns.histplot(similarity_bump_counts['num_similarity_columns_with_bumps'], kde=False, bins=range(0, 10))
plt.title('Histogram and Density for Similarity Column Counts')
plt.xlabel('Different configs number')
plt.ylabel('Frequency')
plt.xticks(range(0, 10, 1))  # Adjust x-axis ticks to show every number
plt.show()

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
import os
# Count the number of clusters in each category
high_threshold = 4
medium_threshold = 1

high_event_clusters = cluster_bump_counts[cluster_bump_counts['num_bumps'] >= high_threshold]
moderate_event_clusters = cluster_bump_counts[(cluster_bump_counts['num_bumps'] >= medium_threshold)\
                                              & (cluster_bump_counts['num_bumps'] < high_threshold)]
low_event_clusters = cluster_bump_counts[cluster_bump_counts['num_bumps'] < medium_threshold]

# Define the sample sizes for each category
total_sample_size = 100
high_event_sample_size = max(25, int((len(high_event_clusters) / len(cluster_bump_counts)) * total_sample_size))
moderate_event_sample_size = max(25, int((len(moderate_event_clusters) / len(cluster_bump_counts)) * total_sample_size))
low_event_sample_size = total_sample_size - high_event_sample_size - moderate_event_sample_size

# Randomly sample clusters within each category
high_event_sample = high_event_clusters.sample(n=high_event_sample_size, random_state=42)
moderate_event_sample = moderate_event_clusters.sample(n=moderate_event_sample_size, random_state=42)
low_event_sample = low_event_clusters.sample(n=low_event_sample_size, random_state=42)

# Combine the samples
sampled_clusters = pd.concat([high_event_sample, moderate_event_sample, low_event_sample])

def assign_category(num_bumps):
    if num_bumps >= high_threshold:
        return 'high'
    elif num_bumps >= medium_threshold:
        return 'medium'
    else:
        return 'low'

# Add cluster category to sampled_clusters
sampled_clusters['category'] = sampled_clusters['num_bumps'].apply(assign_category)

# Merge sampled_clusters with clusters_df to get the full information
sampled_cluster_info = pd.merge(
    sampled_clusters,
    clusters_df,
    on=['topic_id', 'date'],
    how='left'
)

# Save the sampled cluster information to a file
output_file_path = os.path.join(bump_evaluation_path, 'sampled_clusters_info_with_categories.csv')
sampled_cluster_info.to_csv(output_file_path, index=False)

# Verify the sample distribution
print("Sampled cluster information saved to:", output_file_path)
sampled_cluster_info.head(3)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
key_info = sampled_cluster_info[["date","ctfidf_representation","keybert_representation","global_tfidf_representation","repr_docs"]]
# 0 false
# 1 true
# 2 false
# 3
index = 3
for key,value in key_info.iloc[index].items():
    print("Column:",key,", value:", value)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# ### Comparing Bump detection events to GPTs events

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# Ensure uniqueness by dropping duplicates before processing
gpt_events_df = weekly_events_df
event_candidates_df = bump_df[bump_df["has_bumps"]]

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd
import datetime
import numpy as np

# Load the Sentence Transformer model
device = select_gpu_with_most_free_memory()
model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2', device=device)

def join_and_encode(df, text_columns):
    """Join terms in specified columns into a string and batch encode them."""
    for column in tqdm(text_columns, desc = "Processing columns"):
        df[column + '_joined'] = df[column].apply(lambda terms: ', '.join(terms))
    unique_texts = pd.concat([df[col + '_joined'] for col in text_columns]).unique()
    embeddings = model.encode(unique_texts, show_progress_bar=True)
    # Normalize the embeddings
    normalized_embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)
    text_to_embedding = dict(zip(unique_texts, normalized_embeddings))
    for column in text_columns:
        df[column + '_embedding'] = df[column + '_joined'].map(text_to_embedding)
    return df.drop(columns=[col + '_joined' for col in text_columns])

def prepare_embeddings(df, text_columns, date_column):
    # Create a copy of the DataFrame to avoid modifying the original
    df_copy = df[text_columns + ['topic_id', date_column]].copy()
    # Drop duplicates based on 'topic_id' and the date_column to ensure unique pairs
    df_copy = df_copy.drop_duplicates(['topic_id', date_column])
    df_copy.rename(columns={date_column: 'date'}, inplace=True)
    df_copy.set_index(['topic_id', 'date'], inplace=True)
    return join_and_encode(df_copy, text_columns)

# Specify columns for representations
gpt_text_columns = ['ctfidf_representation', 'keybert_representation']
candidate_text_columns = ['ctfidf_representation', 'keybert_representation']

# Prepare embeddings for GPT events and candidates without modifying the original DataFrames
gpt_embeddings_df = prepare_embeddings(weekly_events_df, gpt_text_columns, 'start_date')
candidate_embeddings_df = prepare_embeddings(bump_df[bump_df["has_bumps"]], candidate_text_columns, 'date')

def calculate_cosine_similarity(embedding1, embedding2):
    """Calculate the cosine similarity between two normalized embeddings."""
    return np.dot(embedding1, embedding2)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
def calculate_overlap(tweet_ids1, tweet_ids2):
    set1 = set(tweet_ids1)
    set2 = set(tweet_ids2)
    intersection = len(set1.intersection(set2))
    overlap_left = intersection / len(set1) if set1 else 0
    overlap_right = intersection / len(set2) if set2 else 0
    return overlap_left, overlap_right

# Assuming weekly_events_df is loaded and ready
np.random.seed(42)  # For reproducibility
sampled_gpt_events = gpt_events_df#.sample(n=200, random_state=42)

# Assuming event_candidates_df has the necessary structure and data
configurations = event_candidates_df[['similarity_column', 'alpha']].drop_duplicates()

# Dictionary to store results DataFrames by configuration
config_results = {}

# Loop over bump detection configs
for _, config in tqdm(configurations.iterrows(), desc="Processing configurations"):
    results = []  # This will store results for the current configuration
    filtered_candidates = event_candidates_df[
        (event_candidates_df['similarity_column'] == config['similarity_column']) &
        (event_candidates_df['alpha'] == config['alpha']) &
        (event_candidates_df['has_bumps'] == True)
    ]

    # Preload all candidate embeddings for this config into a dictionary
    candidate_embeddings_ctfidf = candidate_embeddings_df['ctfidf_representation_embedding'].to_dict()
    candidate_embeddings_keybert = candidate_embeddings_df['keybert_representation_embedding'].to_dict()

    for _, gpt_event in sampled_gpt_events.iterrows():
        gpt_embeddings_ctfidf = gpt_embeddings_df.loc[(gpt_event["topic_id"], gpt_event["start_date"]), 'ctfidf_representation_embedding']  
        gpt_embeddings_keybert = gpt_embeddings_df.loc[(gpt_event["topic_id"], gpt_event["start_date"]), 'keybert_representation_embedding']
        
        # Temporal filtering: daily events within the same week as the GPT event
        same_week_candidates = filtered_candidates[
            (filtered_candidates['date'] >= gpt_event['start_date']) &
            (filtered_candidates['date'] <= gpt_event['end_date'])
        ]

        for _, candidate in same_week_candidates.iterrows():
            # Use preloaded embeddings for faster access
            candidate_key = (candidate["topic_id"], candidate["date"])
            candidate_embedding_ctfidf = candidate_embeddings_ctfidf[candidate_key]
            candidate_embedding_keybert = candidate_embeddings_keybert[candidate_key]
            
            cos_sim_ctfidf = calculate_cosine_similarity(candidate_embedding_ctfidf, gpt_embeddings_ctfidf)
            cos_sim_keybert = calculate_cosine_similarity(candidate_embedding_keybert, gpt_embeddings_keybert)
            
            tweet_overlap_left, tweet_overlap_right = calculate_overlap(candidate['tweet_ids'], gpt_event['tweet_ids'])

            # Store the results
            result = {
                'gpt_start_date': gpt_event['start_date'],
                'gpt_end_date': gpt_event['end_date'],
                'gpt_id': gpt_event['topic_id'],
                'gpt_name': gpt_event['event_name'],
                
                'candidate_date': candidate['date'],
                'candidate_id': candidate['topic_id'],
                
                'tweet_overlap_left': tweet_overlap_left,
                'tweet_overlap_right': tweet_overlap_right,
                
                'gpt_keybert': gpt_event['keybert_representation'],
                'candidate_keybert': candidate['keybert_representation'],
                'cos_sim_keybert': cos_sim_keybert,
                
                'gpt_ctfidf': gpt_event['ctfidf_representation'],
                'candidate_ctfidf': candidate['ctfidf_representation'],
                'cos_sim_ctfidf': cos_sim_ctfidf,
            }
            results.append(result)

    # Store each configuration's results in a separate DataFrame within the dictionary
    config_key = (config['similarity_column'], config['alpha'])
    event_mappings = pd.DataFrame(results)
    event_mappings = event_mappings[(event_mappings["tweet_overlap_left"] >= 0.5) | (event_mappings["tweet_overlap_right"] >= 0.5)]
    config_results[config_key] = event_mappings

print("Similarity calculations completed.")

# for _, config in tqdm(configurations.iterrows(), desc="Processing configurations"):
#     results = []  # This will store results for the current configuration
#     filtered_candidates = event_candidates_df[
#         (event_candidates_df['similarity_column'] == config['similarity_column']) &
#         (event_candidates_df['alpha'] == config['alpha']) &
#         (event_candidates_df['has_bumps'] == True)
#     ]

#     for _, gpt_event in sampled_gpt_events.iterrows():
#         gpt_embeddings_ctfidf = gpt_embeddings_df.loc[(gpt_event["topic_id"], gpt_event["start_date"]),\
#                                                       'ctfidf_representation_embedding']  
#         gpt_embeddings_keybert = gpt_embeddings_df.loc[(gpt_event["topic_id"], gpt_event["start_date"]),\
#                                                       'keybert_representation_embedding']
        
#         # Temporal filtering: daily events within the same week as the GPT event
#         same_week_candidates = filtered_candidates[
#             (filtered_candidates['date'] >= gpt_event['start_date']) &
#             (filtered_candidates['date'] <= gpt_event['end_date'])
#         ]

#         for _, candidate in same_week_candidates.iterrows():
            
#             candidate_embeddings_ctfidf = candidate_embeddings_df.loc[(candidate["topic_id"], candidate["date"]),\
#                                                       'ctfidf_representation_embedding']
#             candidate_embeddings_keybert = candidate_embeddings_df.loc[(candidate["topic_id"], candidate["date"]),\
#                                                       'keybert_representation_embedding']
            
            
#             cos_sim_ctfidf = calculate_cosine_similarity(candidate_embeddings_ctfidf, gpt_embeddings_ctfidf)
#             cos_sim_keybert = calculate_cosine_similarity(candidate_embeddings_keybert, gpt_embeddings_keybert)
            
#             tweet_overlap_left, tweet_overlap_right = calculate_overlap(candidate['tweet_ids'], gpt_event['tweet_ids'])

#             # Store the results
#             result = {
#                 'gpt_start_date': gpt_event['start_date'],
#                 'gpt_end_date': gpt_event['end_date'],
#                 'gpt_id': gpt_event['topic_id'],
#                 'gpt_name': gpt_event['event_name'],
                
#                 'candidate_date': candidate['date'],
#                 'candidate_id': candidate['topic_id'],
                
#                 'tweet_overlap_left': tweet_overlap_left,
#                 'tweet_overlap_right': tweet_overlap_right,
                
#                 'gpt_keybert': gpt_event['keybert_representation'],
#                 'candidate_keybert': candidate['keybert_representation'],
#                 'cos_sim_keybert': cos_sim_keybert,
                
#                 'gpt_ctfidf': gpt_event['ctfidf_representation'],
#                 'candidate_ctfidf': candidate['ctfidf_representation'],
#                 'cos_sim_ctfidf': cos_sim_ctfidf,
#             }
#             results.append(result)

#     # Store each configuration's results in a separate DataFrame within the dictionary
#     config_key = (config['similarity_column'], config['alpha'])
#     event_mappings = pd.DataFrame(results)
#     event_mappings = event_mappings[(event_mappings["tweet_overlap_left"]>=0.5)| \
#                                     (event_mappings["tweet_overlap_right"]>=0.5)]
#     config_results[config_key] = event_mappings

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
pd.set_option('display.expand_frame_repr', False)
pd.set_option('max_colwidth', -1)
config_results[('max_sim_global_tfidf_representation_weighted_list', 0.03)].sort_values(by =["tweet_overlap_left","tweet_overlap_right"], ascending = False)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# ### Adding columns for filtering - percentage of time above/below thresholds, variance_factor etc.

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
import warnings
warnings.filterwarnings('ignore')

def add_compliance_metrics(bump_df):
    # Define a function to calculate the compliance metrics along with day distances
    def calculate_metrics(row):
        lower_threshold = row['lower_threshold']
        upper_threshold = row['upper_threshold']
        smoothed_values = row['smoothed_values']

        # Calculate the percentage of values below the lower threshold
        below_threshold_count = sum(value <= lower_threshold for value in smoothed_values)
        percentage_below_lower = (below_threshold_count / len(smoothed_values)) * 100

        # Calculate the percentage of values strictly between the lower and upper thresholds
        between_threshold_count = sum(lower_threshold < value < upper_threshold for value in smoothed_values)
        percentage_between_thresholds = (between_threshold_count / len(smoothed_values)) * 100

        # Calculate the percentage of values above the upper threshold
        above_threshold_count = sum(value > upper_threshold for value in smoothed_values)
        percentage_above_upper = (above_threshold_count / len(smoothed_values)) * 100

        # Calculate the min-max variance factor
        max_value = max(smoothed_values)
        min_value = min(smoothed_values)
        variance = max_value - min_value
        threshold_difference = upper_threshold - lower_threshold
        variance_factor = variance / threshold_difference if threshold_difference != 0 else float('inf')  # Prevent division by zero

        # Calculate day distances for closest and largest bump peaks
        closest_peak_day_distance = abs((row['closest_bump_peak_date'] - row['date']).days)
        largest_peak_day_distance = abs((row['largest_bump_peak_date'] - row['date']).days)

        return pd.Series([percentage_below_lower, percentage_between_thresholds, percentage_above_upper, variance_factor, 
                          closest_peak_day_distance, largest_peak_day_distance],
                         index=['percentage_below_lower', 'percentage_between_thresholds', 'percentage_above_upper', 
                                'variance_factor', 'closest_peak_day_distance', 'largest_peak_day_distance'])

    # Apply the function and add new columns to the DataFrame
    bump_df[['percentage_below_lower', 'percentage_between_thresholds', 'percentage_above_upper', 
             'variance_factor', 'closest_peak_day_distance', 'largest_peak_day_distance']] = bump_df.apply(calculate_metrics, axis=1)
    return bump_df

# Example usage with your DataFrame
event_candidates_df = add_compliance_metrics(event_candidates_df)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
event_candidates_df.columns

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# ### Viusalising distributions of additional columns

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# Setting aesthetics for seaborn
sns.set(style="whitegrid")

def visualize_distributions(data_frame):
    plt.figure(figsize=(18, 12))

    # Histogram and Density Plot for 'percentage_below_lower'
    plt.subplot(2, 3, 1)
    sns.histplot(data_frame['percentage_below_lower'], kde=True, color='blue', bins=30)
    plt.title('Distribution of % Below Lower Threshold')
    plt.xlabel('% of Time Below Lower Threshold')
    plt.ylabel('Frequency')

    # Histogram and Density Plot for 'percentage_between_thresholds'
    plt.subplot(2, 3, 2)
    sns.histplot(data_frame['percentage_between_thresholds'], kde=True, color='purple', bins=30)
    plt.title('Distribution of % Between Thresholds')
    plt.xlabel('% of Time Between Thresholds')
    plt.ylabel('Frequency')

    # Histogram and Density Plot for 'percentage_above_upper'
    plt.subplot(2, 3, 3)
    sns.histplot(data_frame['percentage_above_upper'], kde=True, color='red', bins=30)
    plt.title('Distribution of % Above Upper Threshold')
    plt.xlabel('% of Time Above Upper Threshold')
    plt.ylabel('Frequency')

    # Histogram and Density Plot for 'variance_factor'
    plt.subplot(2, 3, 4)
    sns.histplot(data_frame['variance_factor'], kde=True, color='green', bins=30)
    plt.title('Distribution of Variance Factor')
    plt.xlabel('Min-Max to Threshold Difference Factor')
    plt.ylabel('Frequency')

    # Boxplots for detailed distribution analysis
    plt.subplot(2, 3, 5)
    sns.boxplot(y=data_frame['percentage_below_lower'], color='cyan')
    plt.title('Boxplot of % Below Lower Threshold')
    plt.ylabel('% of Time Below Lower Threshold')

    plt.subplot(2, 3, 6)
    sns.boxplot(y=data_frame['variance_factor'], color='lightgreen')
    plt.title('Boxplot of Variance Factor')
    plt.ylabel('Min-Max to Threshold Difference Factor')

    plt.tight_layout()
    plt.show()

# Assuming 'event_candidates_df' is your DataFrame
visualize_distributions(event_candidates_df)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# ### VISUALISING TIME SERIES WITH BUMPS

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
def visualize_bumps(bump_df, N, sim_column, alpha = 0.05, min_date =  datetime.date(2019,1,1),\
                    max_date= datetime.date(2020,4,24)):
    # Extract the global start and end dates from the DataFrame
    global_start_date = datetime.date(2019,1,1) #bump_df['date'].min()
    global_end_date = datetime.date(2020,4,24) #bump_df['date'].max()

    # Generate a timeline from the global start to end date
    timeline = pd.date_range(start=global_start_date, end=global_end_date)
    
    #
    filtered_df = bump_df[(bump_df["similarity_column"] == sim_column)\
                          &(bump_df["alpha"] == alpha) \
                          &(bump_df["date"] >=min_date)\
                         &(bump_df["date"] <=max_date)]
    
    # Sample N random clusters
    sampled_clusters = filtered_df.sample(n=N, replace=False)

    # Plotting
    fig, axs = plt.subplots(N, 1, figsize=(10, N * 4))

    if N == 1:
        axs = [axs]  # Make sure axs is iterable for a single plot

    for ax, (_, row) in zip(axs, sampled_clusters.iterrows()):
        # Extract smoothed values from the DataFrame
        smoothed_values = row['smoothed_values']

        # Plot original and smoothed values
        ax.plot(timeline, smoothed_values, label=f'LOESS {alpha}', color='red')

        # Highlight the closest bump peak date
        if row['closest_bump_peak_date']:
            ax.axvline(x=row['closest_bump_peak_date'], color='green', linestyle='--', label='Closest Bump Peak')
        
        ax.axvline(x=row['date'], color='orange', linestyle='--', label='Date of topic')
        ax.hlines(y=row["lower_threshold"], xmin = timeline.min(),xmax = timeline.max(),linewidth=1, color='black')
        ax.hlines(y=row["upper_threshold"], xmin = timeline.min(),xmax = timeline.max(),linewidth=1, color='black')

        ax.set_title(f"Topic ID: {row['topic_id']} - Date: {row['date']} - {row['global_tfidf_representation']}")
        ax.set_xlabel('Timeline Date')
        ax.set_ylabel('Smoothed Value')
        ax.legend()

    plt.tight_layout()
    plt.show()

# Example usage
# visualize_bumps(event_candidates_df, N=5, sim_column = "max_sim_global_tfidf_representation_weighted_list", 
#                 alpha = 0.03,
#                 min_date =  datetime.date(2019,1,1),
#                 max_date =  datetime.date(2020,4,24))

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# ### Example of filtering based on time spent in certain inteval, variance_factor and bump-date distance

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# Now you can easily filter and analyze the DataFrame based on the new metrics
max_delta_days = 15
filtered_bump_df = event_candidates_df[(event_candidates_df['percentage_below_lower'] >= 75) \
                                       & (event_candidates_df['variance_factor'] <= 5) \
                                      & (event_candidates_df['closest_peak_day_distance'] <= max_delta_days)] 
print(f"Filtered DataFrame has {len(filtered_bump_df)} rows out of {len(event_candidates_df)} total rows.")

# Optionally, you can inspect the DataFrame to see the distribution or summary statistics
print(filtered_bump_df[['percentage_below_lower', 'variance_factor']].describe())

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
print("Number of bumps: \n", filtered_bump_df.total_bumps.value_counts())
bump_counts = filtered_bump_df.groupby(['similarity_column', 'alpha']).size().reset_index(name='count')
print("\nNumber of event candidates per combination: \n")
bump_counts.sort_values(by="count", ascending = False)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
bump_counts.sort_values(by="count")

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# ### Experimenting with graphs for different filters

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
visualize_bumps(bump_df = filtered_bump_df,
                N=5, 
                sim_column = "max_sim_global_tfidf_representation_weighted_list", 
                alpha = 0.03,
                min_date =  datetime.date(2020,2,1),
                max_date =  datetime.date(2020,4,24))