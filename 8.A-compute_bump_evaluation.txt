# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# -*- coding: utf-8 -*-
import dataiku
import pandas as pd, numpy as np
from dataiku import pandasutils as pdu
import matplotlib.pyplot as plt
import seaborn as sns
import random
import datetime
from utils import (
    select_gpu_with_most_free_memory,
    load_experiment_jsons,
    load_experiment_objects,
    get_current_time_and_unix_timestamp,
    get_newest_json,
    get_unique_dictionaries,
    save_combination_list,
)
import ast
from tqdm import tqdm

# Input folders
ed_clusters_with_reps = dataiku.Folder("oiEvOl5p")
ed_clusters_with_reps_path = ed_clusters_with_reps.get_path()

ed_similarity_scores = dataiku.Folder("0JFzvl2d")
ed_similarity_scores_path= ed_similarity_scores.get_path()

ed_similarity_scores_processed = dataiku.Folder("hZfSC2LV")
ed_similarity_scores_processed_path = ed_similarity_scores_processed.get_path()

# Input datasets
weekly_events_merged = dataiku.Dataset("weekly_events_merged")
weekly_events_df = weekly_events_merged.get_dataframe()

key_events_per_period = dataiku.Dataset("key_events_per_period")
key_events_df = key_events_per_period.get_dataframe()

# Fixing lists
weekly_events_df['representative_docs'] = weekly_events_df['representative_docs'].apply(ast.literal_eval)
weekly_events_df['ctfidf_representation'] = weekly_events_df['ctfidf_representation'].apply(ast.literal_eval)
weekly_events_df['keybert_representation'] = weekly_events_df['keybert_representation'].apply(ast.literal_eval)
weekly_events_df['tweet_ids'] = weekly_events_df['tweet_ids'].apply(ast.literal_eval)

# Fixing date columns to get rid of hours, mins, seconds
weekly_events_df['start_date'] = pd.to_datetime(weekly_events_df['start_date']).dt.date
weekly_events_df['end_date'] = pd.to_datetime(weekly_events_df['end_date']).dt.date

# Fixing date columns to get rid of hours, mins, seconds
key_events_df['start_date'] = pd.to_datetime(key_events_df['start_date']).dt.date
key_events_df['end_date'] = pd.to_datetime(key_events_df['end_date']).dt.date

key_events_df.rename(columns={"key_event_id": "topic_id"},inplace = True)
key_events_df = key_events_df.merge(weekly_events_df, on=['topic_id', 'start_date'],suffixes=('', '_y'))
key_events_df.drop(key_events_df.filter(regex='_y$').columns, axis=1, inplace=True)

# Output folder
bump_evaluation = dataiku.Folder("bFPFsUEv")
bump_evaluation_path = bump_evaluation.get_path()

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
print("Number of key ground truth events: ",len(key_events_df))
key_events_df.head(1)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
experiment_jsons = load_experiment_jsons(
                            root_folder_path = ed_similarity_scores_processed_path,
                            dataset_name = "",
                            experiment_details_subfolder = "bump_detection_experiment_details")
# Add filter if necessary
filtered_jsons = experiment_jsons
# filtered_jsons

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# ### LOADING ALL STRUCTURES FOR CURRENT EXPERIMENT

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
for curr_json in filtered_jsons:
    clusters_df  = load_experiment_objects(experiment_jsons = [curr_json],
                            file_path_key = "clustering_save_location")[0]
    
    # Ensure 'clusters_df' has the correct column order
    primary_columns = ['topic_id', 'date', 'cluster_size']
    secondary_columns = [col for col in clusters_df.columns if col not in primary_columns]
    clusters_df = clusters_df[primary_columns + secondary_columns]
    
#     similarity_scores = load_experiment_objects(experiment_jsons = [curr_json],
#                             file_path_key = "similarity_save_location")[0]
    
    bump_df  = load_experiment_objects(experiment_jsons = [curr_json],
                            file_path_key = "bump_save_location")[0]
    
    bump_df = bump_df.merge(clusters_df, on=["topic_id","date"])
    event_candidates_df = bump_df[bump_df["has_bumps"]]

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# ### Extracting ground_truth events

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# FOR EACH TOPIC WE COUNT THE NUMBER OF CONFIGS/SIM_COLUMNS FOR WHICH IT HAS CLOSE BUMPS
max_delta_days = 3

# Create a condition to filter rows - has no bump or has bump close by
condition = (bump_df['has_bumps']==False) \
            | (abs((bump_df['closest_bump_peak_date'] - bump_df['date']).apply(lambda x: x.days)) <= max_delta_days) 

# Filter the DataFrame based on the condition
close_bumps_df = bump_df[condition]

# Display the filtered DataFrame
close_bumps_df.head(3)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# 1. Counting the number of rows (configs) where has_bumps is True for each cluster (grouped by date and topic_id)
cluster_bump_counts = close_bumps_df.groupby(['date', 'topic_id'])['has_bumps'].sum().reset_index()
cluster_bump_counts.rename(columns={'has_bumps': 'num_bumps'}, inplace=True)

# 2. Counting the number of unique similarity_column values where at least one config has has_bumps as True
# First, create a boolean DataFrame to indicate if there's any bump for each similarity_column within each cluster
bump_presence_per_similarity = close_bumps_df.groupby(['date', 'topic_id', 'similarity_column'])['has_bumps'].any().reset_index()

# Then, count the unique similarity_column values where has_bumps is True for each cluster
similarity_bump_counts = bump_presence_per_similarity.groupby(['date', 'topic_id'])['has_bumps'].sum().reset_index()
similarity_bump_counts.rename(columns={'has_bumps': 'num_similarity_columns_with_bumps'}, inplace=True)

# Ensuring all clusters are included by merging with the original clusters
all_clusters = close_bumps_df[['date', 'topic_id']].drop_duplicates()
similarity_bump_counts = all_clusters.merge(similarity_bump_counts, on=['date', 'topic_id'], how='left').fillna(0)
similarity_bump_counts['num_similarity_columns_with_bumps'] = similarity_bump_counts['num_similarity_columns_with_bumps'].astype(int)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
#cluster_bump_counts.sort_values(by = "num_bumps", ascending = False)
#similarity_bump_counts.sort_values(by = "num_similarity_columns_with_bumps", ascending = False)

# Plotting Histogram and Density for Config Counts
plt.figure(figsize=(12, 6))
sns.histplot(cluster_bump_counts['num_bumps'], kde=False, bins=range(0, 34))
plt.title('Histogram and Density for Config Counts')
plt.xlabel('Config number')
plt.ylabel('Frequency')
plt.xticks(range(0, 34, 2))  # Adjust x-axis ticks to show every second number
plt.show()

# Plotting Histogram and Density for Similarity Column Counts
plt.figure(figsize=(12, 6))
sns.histplot(similarity_bump_counts['num_similarity_columns_with_bumps'], kde=False, bins=range(0, 10))
plt.title('Histogram and Density for Similarity Column Counts')
plt.xlabel('Different configs number')
plt.ylabel('Frequency')
plt.xticks(range(0, 10, 1))  # Adjust x-axis ticks to show every number
plt.show()

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
import os
# Count the number of clusters in each category
high_threshold = 3
medium_threshold = 1

high_event_clusters = cluster_bump_counts[cluster_bump_counts['num_bumps'] >= high_threshold]
moderate_event_clusters = cluster_bump_counts[(cluster_bump_counts['num_bumps'] >= medium_threshold)\
                                              & (cluster_bump_counts['num_bumps'] < high_threshold)]
low_event_clusters = cluster_bump_counts[cluster_bump_counts['num_bumps'] < medium_threshold]

# Define the sample sizes for each category
total_sample_size = 100
high_event_sample_size = max(25, int((len(high_event_clusters) / len(cluster_bump_counts)) * total_sample_size))
moderate_event_sample_size = max(25, int((len(moderate_event_clusters) / len(cluster_bump_counts)) * total_sample_size))
low_event_sample_size = total_sample_size - high_event_sample_size - moderate_event_sample_size

# Randomly sample clusters within each category
high_event_sample = high_event_clusters.sample(n=high_event_sample_size, random_state=42)
moderate_event_sample = moderate_event_clusters.sample(n=moderate_event_sample_size, random_state=42)
low_event_sample = low_event_clusters.sample(n=low_event_sample_size, random_state=42)

# Combine the samples
sampled_clusters = pd.concat([high_event_sample, moderate_event_sample, low_event_sample])

def assign_category(num_bumps):
    if num_bumps >= high_threshold:
        return 'high'
    elif num_bumps >= medium_threshold:
        return 'medium'
    else:
        return 'low'

# Add cluster category to sampled_clusters
sampled_clusters['category'] = sampled_clusters['num_bumps'].apply(assign_category)

# Merge sampled_clusters with clusters_df to get the full information
sampled_cluster_info = pd.merge(
    sampled_clusters,
    clusters_df,
    on=['topic_id', 'date'],
    how='left'
)

# Save the sampled cluster information to a file
#output_file_path = os.path.join(bump_evaluation_path, 'sampled_clusters_info_with_categories.csv')
#sampled_cluster_info.to_csv(output_file_path, index=False)

# Verify the sample distribution
#print("Sampled cluster information saved to:", output_file_path)
sampled_cluster_info.head(3)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# sample_pickle_path = os.path.join(bump_evaluation_path, 'sampled_clusters_info_with_categories.pkl')
# sampled_cluster_info.to_pickle(sample_pickle_path)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
sample_pickle_path = os.path.join(bump_evaluation_path, 'sampled_clusters_info_with_categories.pkl')
sampled_cluster_info =  pd.read_pickle(sample_pickle_path)
sampled_cluster_info.head(3)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
notes = [
(0, False, "SPD possibly handing over passwords through legislation, no clear event or reactions"),
(1, False, "gratitude towards frontline workers during the COVID-19 crisis, particularly around March 20, 2020"),
(2, True, "75th anniversary of the liberation of Auschwitz"),
(3, True, "Iranian retaliatory strikes against US military bases, which occurred shortly after the US drone strike that killed Qasem Soleimani"),
(4, False, "youtuber Rezo goes viral for criticising all german political parties and claiming they are creating a divide"),
(5, False, "discussion about transition in transport, only recent info is on lack of train drivers"),
(6, True, "humanitarian crisis in the Mediterranean, particularly concerning the rescue and distribution of migrants and refugees. immediate rescue needs and alleviate pressure on frontline states like Italy and Malta."),
(7, True, "initiative proposed by Defense Minister Kramp-Karrenbauer regarding the establishment of an international security zone in northern Syria. It reflects efforts to address the humanitarian crisis and stabilize the conflict-ridden region"),
(8, True, "debates about reform to childrens benefits and general welfare proposed by Minister Franziska Giffey"),
(9, False, "debates surrounding proposals for expropriation as a solution to the housing affordability crisis"),
(10, False, "basic disagreements around government expenses between socialist and liberal parties, no clear event"),
(11, True, "unveiling and public reaction to Germany's climate package in 2019, adopted 10 days later"),
(12, False, "parliamentary debate surrounding economic policies, specifically concerning the social market economy, VAT fraud, VAT fraud started being discussed in the media at this period, but no convictions until years later"),
(13, False, "discussion about tax policy, and issues with tax evasion"),
(14, False, "discussion about antisemitism within the Muslim community"),
(15, False, "reflects general ongoing policy discussions and advocacy efforts regarding transportation and climate goals"),
(16, True, "reaction to terrorist attack in Halle synagogue Germany, mentions antisemitism and right-wind extremists"),
(17, False, "ongoing societal issues and discussions around job security - no clear event"),
(18, True, "war in northern Syria, proposals for a UN-led security zone and the responses from various European countries"),
(19, False, "ongoing discussions and debates related to transportation policies, environmental concerns, and political accountability"),
(20, False, "controversy surrounding plans for deportations to Syria due to concerns and human rights considerations"),
(21, False, "primarily focus on the political deadlock and controversies surrounding the expansion of renewable energies, particularly wind and solar power"),
(22, False, "debates within the German political landscape concerning taxation policies"),
(23, False, "debates regarding the direction and effectiveness of energy policies in achieving climate targets."),
(24, False, "discussions and legislative efforts in Germany regarding the rise of right-wing extremism and antisemitism"),
(25, False, "40 years of the green party "),
(26, False, "discussions about environmental policies, motivated by Wildfires in Australia, but they are not the focus"),
(27, True, "70th anniversary of the Council of Europe "),
(28, True, "reaction to recent murder of Walter Lubcke and the general rise of right-wing extremism"),
(29, False, "mixed cluster, no clear place/event that it is tied to, mentions of earlier (2018) Marches of Pegida/AfD"),
(30, True, "Christine Lambrecht's speech condemning right-wing extremism and violence in the Bundestag, addressing issues related to the NSU and specific incidents like the murder of Walter Lübcke."),
(31, True, "the activities and statements of the AfD faction regarding their visit to Syria and the subsequent release of a documentary"),
(32, True, "70th anniversary of the Council of Europe "),
(33, True, "rejection of expansion of CO2 reduction strategies in the Bundestag - as part of broader EU emssions scheme"),
(34, True, "criticism and reactions to Ursula von der Leyen's appointment as Defense Minister despite previous statements about not seeking such positions, reflecting on credibility and political strategies"),
(35, False, "general discussion about taxation, digitalization, and social issues like poverty and inequality"),
(36, False, "mixed cluster - random greetings from Hamburg and issues with providing Ventilators during Covid-19"),
(37, True, "reaction to classification of wirsindmehr protests as left-wing extremism in Saxony on May 16, 2019. ties in to general issues of neo-nazis in eastern germany"),
(38, False, "debates surrounding tax policies, particularly the push for tax reductions and economic implications"),
(39, True, "discussion about the types and availabilty of masks during the Covid-19 pandemic"),
(40, False, "small cluster, various topics such as parliamentary debates (Bundestag), personal opinions and experiences, calls for action regarding issues like Duogynon, and inquiries about political responses"),
(41, True, "Day of Persons with Disabilities - discussions about the ongoing challenges faced by people with disabilities, efforts to improve access to education, employment, and healthcare, and the importance of raising awareness and societal support"),
(42, False, "mixed topic - reactions to SPDs proposed changes to Pension policy, accusations of sexism within the party as one of the female members of SPD was ignored"),
(43, False, "general campaign statements 3 weeks before EU 2019 elections, AfD calls for more independence from Brussels"),
(44, False, "impact of wage policies on part-time workers, criticisms of current thresholds, and proposals for dynamic adjustments"),
(45, False, "debates around the fairness of pension schemes, criticisms of proposed policies, and the impact of pension reforms"),
(46, True, "International Workers Day (May 1st), themes of solidarity, social justice, and European integration"),
(47, True, "Day against the Use of Child Soldiers (RedHandDay) - focus on raising awareness, advocating against the use of child soldiers in armed conflicts, and calling for global action to address this humanitarian crisis"),
(48, False, "introduction and debates surrounding the Grundrente policy in Germany, highlighting discussions on fairness"),
(49, False, "discussion of participation of women in parlament and how to achieve parity, tied in to 100 years of women's suffrage"),
(50, False, "range of issues within education, consumer rights and general economic matters being discussed as 2019 ends"),
(51, False, "mentions of Advent activities (Adventssingen) and reflections on historical writings by Dietrich Bonhoeffer, marking 75 years since their creation."),
(52, False, "internal discussion about reorganization within the SPD"),
(53, False, "initiatives on icreasing usage of wind energy in 2020"),
(54, False, "solving legal ambiguities regarding the Gemeinsame Terrorismusabwehrzentrum (GTAZ) as of November 2019."),
(55, True, "abortion rights -calls for protests on January 26th at Rosa-Luxemburg-Platz in Berlin against Paragraph 219a, highlighting the importance of sexual self-determination"),
(56, False, "May 8th anniversary (end of WW2), general reflections "),
(57, True, "escalation of Covid-19 pandemic, border closure, debates over border policies and closures, economic impacts, and efforts to combat misinformation"),
(58, False, "range of discussions on human rights, historical events like the peaceful revolution in Berlin, legislative actions needed regarding supply chains, and Germany's stance on international human rights issues"),
(59, False, "range of discussions - establishment of the German Foundation for Engagement and Volunteerism, Franziska Giffey's role as the Minister for Family Affairs"),
(60, True, "humanitarian crisis in the Mediterranean Sea, focusing on sea rescue operations"),
(61, False, "discussions around climate change mitigation strategies focusing on tree planting initiatives, forest management reforms"),
(62, False, "mixed cluster no clear topic, personal annegdotes"),
(63, False, "reactions to general statements made during the 2020 Munich Security Council"),
(64, False, "general discussions on human rights, no clear motivation"),
(65, False, "discussion series denoted as '3K20', where participants or speakers express their views on standing firm with their opinions and goals"),
(66, False, "initiatives and debates around traffic safety and policy reform, particularly regarding cycling infrastructure and speed limits"),
(67, False, "general discussions about Covid-19 pandemic, no clear development"),
(68, False, "discussions about railway issues in hanburg and local events from the AfD"),
(69, False, "ongoing discussions and activities related to healthcare policy, emergency services, and digitalization efforts in healthcare"),
(70, False," remembering past tragedies caused by right-wing extremism, assessing current threats, and advocating for collective action against such ideologies"),
(71, False, "debates ranging from electoral mechanics to ideological positioning and public perception of political figures"),
(72, False, "active debates concerning the Identitarian Movement's role in politics and society, with a focus on its classification as a right-wing extremist group"),
(73, False, "30th anniversary Tiananmen Square Massacre and discussions on relations with china"),
(74, True, "reactions to Brandenburg and Saxony state elections in 2019"),
(75, False, "discussions on climate policy, climate goals, and the urgency of addressing climate change"),
(76, False, "casual conversations"),
(77, False, "statements about Macrons calls for EU reform ahead of EU elections"),
(78, False, "criticism about fascist tendencies calling on the recent Thuringen election results (1 month prior)"),
(79, False, "criticism and debates surrounding the actions and statements of CDU politicians and statements about possible alliances in Thuringen"),
(80, True, "results of presidential elections in Slovakia, seen as a positive development for liberal and pro-European movements across the continent"),
(81, False, "analysis of public sentiment and political responses to media reports, polls, and surveys on climate change issues"),
(82, False, "mixed - human rights advocacy based on 25 years of Rwandan genocide, discussion about rights on International Roma Day"),
(83, False, "discussion about global vaccine development, and making sure that it is available to all, no clear events"),
(84, False, "Participation of influencers and politicians like Philipp Amthor in discussions regarding youth involvement in politics and future political rights"),
(85, False, "mix of human rights issues, domestic policy, and recent diplomatic visits by Frank-Walter Steinmeier"),
(86, False, "legal and policy debates,  related to constitutional interpretations or legislative proposals, no clear goal or proposal"),
(87, False, "mix of activities, discussions, personal reflections in Düsseldorf and Berlin, involving local leaders, visitors, and community members."),
(88, False, "general discussions about migration and human rights, internal debates within the AfD"),
(89, False, "mix of Discussions and engagements by political figures like Jürgen Klein, focusing on regional agendas and party activities and sports achievemts"),
(90, True, "reaction to sentence of Ojub Titiev, calls for fair trials and international pressure on Russia to uphold human rights"),
(91, False, "mix of many local events"),
(92, False, "mix of internal AfD elections and meeting of ministers"),
(93, False, "discussions on healthcare policy, patient rights, and decision-making processes"),
(94, False, "70th anniversary of NATO and discussion about Germany's role within the organisation"),
(95, False," small mixed cluster no specific context or details"),
(96, False, "support for EU accession talks involving North Macedonia and Albania from Germany"),
(97, False, "discussion about possible price increase on CO2 emissions"),
(98, False, "mix of criticism of Minister Andreas Scheuers plan on road toll/tax as well as changes in Die Linke and SPD"),
(99, False, "criticism of AfD and Die Linke on their stance for criminalization of sea rescue operations (Seenotrettung).")
]

notes_df = pd.DataFrame(notes, columns=['index', 'label', 'label_comment'])
sampled_cluster_info['label'] = notes_df['label']
sampled_cluster_info['label_comment'] = notes_df['label_comment']
sampled_cluster_info.head(3)
# key_info = sampled_cluster_info[["date","ctfidf_representation","keybert_representation","global_tfidf_representation","repr_docs"]]
# index = 2
# for key,value in key_info.iloc[index].items():
#     print("Column:",key,", value:", value)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
clusters_with_labels_path = os.path.join(bump_evaluation_path, 'sampled_clusters_with_labels.pkl')
#sampled_cluster_info.to_pickle(clusters_with_labels_path)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
sampled_cluster_info = pd.read_pickle(clusters_with_labels_path)
sampled_cluster_info

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# ### Comparing Bump detection events to GPTs events

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# Ensure uniqueness by dropping duplicates before processing
gpt_events_df = weekly_events_df
event_candidates_df = bump_df[bump_df["has_bumps"]]

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd
import datetime
import numpy as np

# Load the Sentence Transformer model
device = select_gpu_with_most_free_memory()
model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2', device=device)

def join_and_encode(df, text_columns):
    """Join terms in specified columns into a string and batch encode them."""
    for column in tqdm(text_columns, desc = "Processing columns"):
        df[column + '_joined'] = df[column].apply(lambda terms: ', '.join(terms))
    unique_texts = pd.concat([df[col + '_joined'] for col in text_columns]).unique()
    embeddings = model.encode(unique_texts, show_progress_bar=True)
    # Normalize the embeddings
    normalized_embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)
    text_to_embedding = dict(zip(unique_texts, normalized_embeddings))
    for column in text_columns:
        df[column + '_embedding'] = df[column + '_joined'].map(text_to_embedding)
    return df.drop(columns=[col + '_joined' for col in text_columns])

def prepare_embeddings(df, text_columns, date_column):
    # Create a copy of the DataFrame to avoid modifying the original
    df_copy = df[text_columns + ['topic_id', date_column]].copy()
    # Drop duplicates based on 'topic_id' and the date_column to ensure unique pairs
    df_copy = df_copy.drop_duplicates(['topic_id', date_column])
    df_copy.rename(columns={date_column: 'date'}, inplace=True)
    df_copy.set_index(['topic_id', 'date'], inplace=True)
    return join_and_encode(df_copy, text_columns)

# Specify columns for representations
gpt_text_columns = ['ctfidf_representation', 'keybert_representation']
candidate_text_columns = ['ctfidf_representation', 'keybert_representation']

# Prepare embeddings for GPT events and candidates without modifying the original DataFrames
gpt_embeddings_df = prepare_embeddings(weekly_events_df, gpt_text_columns, 'start_date')
candidate_embeddings_df = prepare_embeddings(bump_df[bump_df["has_bumps"]], candidate_text_columns, 'date')

def calculate_cosine_similarity(embedding1, embedding2):
    """Calculate the cosine similarity between two normalized embeddings."""
    return np.dot(embedding1, embedding2)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
def calculate_overlap(tweet_ids1, tweet_ids2):
    set1 = set(tweet_ids1)
    set2 = set(tweet_ids2)
    intersection = len(set1.intersection(set2))
    overlap_left = intersection / len(set1) if set1 else 0
    overlap_right = intersection / len(set2) if set2 else 0
    return overlap_left, overlap_right

# Assuming weekly_events_df is loaded and ready
np.random.seed(42)  # For reproducibility
sampled_gpt_events = gpt_events_df#.sample(n=200, random_state=42)

# Assuming event_candidates_df has the necessary structure and data
configurations = event_candidates_df[['similarity_column', 'alpha']].drop_duplicates()

# Dictionary to store results DataFrames by configuration
config_results = {}

# Loop over bump detection configs
for _, config in tqdm(configurations.iterrows(), desc="Processing configurations"):
    results = []  # This will store results for the current configuration
    filtered_candidates = event_candidates_df[
        (event_candidates_df['similarity_column'] == config['similarity_column']) &
        (event_candidates_df['alpha'] == config['alpha']) &
        (event_candidates_df['has_bumps'] == True)
    ]

    # Preload all candidate embeddings for this config into a dictionary
    candidate_embeddings_ctfidf = candidate_embeddings_df['ctfidf_representation_embedding'].to_dict()
    candidate_embeddings_keybert = candidate_embeddings_df['keybert_representation_embedding'].to_dict()

    for _, gpt_event in sampled_gpt_events.iterrows():
        gpt_embeddings_ctfidf = gpt_embeddings_df.loc[(gpt_event["topic_id"], gpt_event["start_date"]), 'ctfidf_representation_embedding']  
        gpt_embeddings_keybert = gpt_embeddings_df.loc[(gpt_event["topic_id"], gpt_event["start_date"]), 'keybert_representation_embedding']
        
        # Temporal filtering: daily events within the same week as the GPT event
        same_week_candidates = filtered_candidates[
            (filtered_candidates['date'] >= gpt_event['start_date']) &
            (filtered_candidates['date'] <= gpt_event['end_date'])
        ]

        for _, candidate in same_week_candidates.iterrows():
            # Use preloaded embeddings for faster access
            candidate_key = (candidate["topic_id"], candidate["date"])
            candidate_embedding_ctfidf = candidate_embeddings_ctfidf[candidate_key]
            candidate_embedding_keybert = candidate_embeddings_keybert[candidate_key]
            
            cos_sim_ctfidf = calculate_cosine_similarity(candidate_embedding_ctfidf, gpt_embeddings_ctfidf)
            cos_sim_keybert = calculate_cosine_similarity(candidate_embedding_keybert, gpt_embeddings_keybert)
            
            tweet_overlap_left, tweet_overlap_right = calculate_overlap(candidate['tweet_ids'], gpt_event['tweet_ids'])

            # Store the results
            result = {
                'gpt_start_date': gpt_event['start_date'],
                'gpt_end_date': gpt_event['end_date'],
                'gpt_id': gpt_event['topic_id'],
                'gpt_name': gpt_event['event_name'],
                
                'candidate_date': candidate['date'],
                'candidate_id': candidate['topic_id'],
                
                'tweet_overlap_left': tweet_overlap_left,
                'tweet_overlap_right': tweet_overlap_right,
                
                'gpt_keybert': gpt_event['keybert_representation'],
                'candidate_keybert': candidate['keybert_representation'],
                'cos_sim_keybert': cos_sim_keybert,
                
                'gpt_ctfidf': gpt_event['ctfidf_representation'],
                'candidate_ctfidf': candidate['ctfidf_representation'],
                'cos_sim_ctfidf': cos_sim_ctfidf,
            }
            results.append(result)

    # Store each configuration's results in a separate DataFrame within the dictionary
    config_key = (config['similarity_column'], config['alpha'])
    event_mappings = pd.DataFrame(results)
    event_mappings = event_mappings[(event_mappings["tweet_overlap_left"] >= 0.5) | (event_mappings["tweet_overlap_right"] >= 0.5)]
    config_results[config_key] = event_mappings

print("Similarity calculations completed.")

# for _, config in tqdm(configurations.iterrows(), desc="Processing configurations"):
#     results = []  # This will store results for the current configuration
#     filtered_candidates = event_candidates_df[
#         (event_candidates_df['similarity_column'] == config['similarity_column']) &
#         (event_candidates_df['alpha'] == config['alpha']) &
#         (event_candidates_df['has_bumps'] == True)
#     ]

#     for _, gpt_event in sampled_gpt_events.iterrows():
#         gpt_embeddings_ctfidf = gpt_embeddings_df.loc[(gpt_event["topic_id"], gpt_event["start_date"]),\
#                                                       'ctfidf_representation_embedding']  
#         gpt_embeddings_keybert = gpt_embeddings_df.loc[(gpt_event["topic_id"], gpt_event["start_date"]),\
#                                                       'keybert_representation_embedding']
        
#         # Temporal filtering: daily events within the same week as the GPT event
#         same_week_candidates = filtered_candidates[
#             (filtered_candidates['date'] >= gpt_event['start_date']) &
#             (filtered_candidates['date'] <= gpt_event['end_date'])
#         ]

#         for _, candidate in same_week_candidates.iterrows():
            
#             candidate_embeddings_ctfidf = candidate_embeddings_df.loc[(candidate["topic_id"], candidate["date"]),\
#                                                       'ctfidf_representation_embedding']
#             candidate_embeddings_keybert = candidate_embeddings_df.loc[(candidate["topic_id"], candidate["date"]),\
#                                                       'keybert_representation_embedding']
            
            
#             cos_sim_ctfidf = calculate_cosine_similarity(candidate_embeddings_ctfidf, gpt_embeddings_ctfidf)
#             cos_sim_keybert = calculate_cosine_similarity(candidate_embeddings_keybert, gpt_embeddings_keybert)
            
#             tweet_overlap_left, tweet_overlap_right = calculate_overlap(candidate['tweet_ids'], gpt_event['tweet_ids'])

#             # Store the results
#             result = {
#                 'gpt_start_date': gpt_event['start_date'],
#                 'gpt_end_date': gpt_event['end_date'],
#                 'gpt_id': gpt_event['topic_id'],
#                 'gpt_name': gpt_event['event_name'],
                
#                 'candidate_date': candidate['date'],
#                 'candidate_id': candidate['topic_id'],
                
#                 'tweet_overlap_left': tweet_overlap_left,
#                 'tweet_overlap_right': tweet_overlap_right,
                
#                 'gpt_keybert': gpt_event['keybert_representation'],
#                 'candidate_keybert': candidate['keybert_representation'],
#                 'cos_sim_keybert': cos_sim_keybert,
                
#                 'gpt_ctfidf': gpt_event['ctfidf_representation'],
#                 'candidate_ctfidf': candidate['ctfidf_representation'],
#                 'cos_sim_ctfidf': cos_sim_ctfidf,
#             }
#             results.append(result)

#     # Store each configuration's results in a separate DataFrame within the dictionary
#     config_key = (config['similarity_column'], config['alpha'])
#     event_mappings = pd.DataFrame(results)
#     event_mappings = event_mappings[(event_mappings["tweet_overlap_left"]>=0.5)| \
#                                     (event_mappings["tweet_overlap_right"]>=0.5)]
#     config_results[config_key] = event_mappings

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
pd.set_option('display.expand_frame_repr', False)
pd.set_option('max_colwidth', -1)
config_results[('max_sim_global_tfidf_representation_weighted_list', 0.03)].sort_values(by =["tweet_overlap_left","tweet_overlap_right"], ascending = False)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# ### Adding columns for filtering - percentage of time above/below thresholds, variance_factor etc.

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
import warnings
warnings.filterwarnings('ignore')

def add_compliance_metrics(bump_df):
    # Define a function to calculate the compliance metrics along with day distances
    def calculate_metrics(row):
        lower_threshold = row['lower_threshold']
        upper_threshold = row['upper_threshold']
        smoothed_values = row['smoothed_values']

        # Calculate the percentage of values below the lower threshold
        below_threshold_count = sum(value <= lower_threshold for value in smoothed_values)
        percentage_below_lower = (below_threshold_count / len(smoothed_values)) * 100

        # Calculate the percentage of values strictly between the lower and upper thresholds
        between_threshold_count = sum(lower_threshold < value < upper_threshold for value in smoothed_values)
        percentage_between_thresholds = (between_threshold_count / len(smoothed_values)) * 100

        # Calculate the percentage of values above the upper threshold
        above_threshold_count = sum(value > upper_threshold for value in smoothed_values)
        percentage_above_upper = (above_threshold_count / len(smoothed_values)) * 100

        # Calculate the min-max variance factor
        max_value = max(smoothed_values)
        min_value = min(smoothed_values)
        variance = max_value - min_value
        threshold_difference = upper_threshold - lower_threshold
        variance_factor = variance / threshold_difference if threshold_difference != 0 else float('inf')  # Prevent division by zero

        # Calculate day distances for closest and largest bump peaks
        closest_peak_day_distance = abs((row['closest_bump_peak_date'] - row['date']).days)
        largest_peak_day_distance = abs((row['largest_bump_peak_date'] - row['date']).days)

        return pd.Series([percentage_below_lower, percentage_between_thresholds, percentage_above_upper, variance_factor, 
                          closest_peak_day_distance, largest_peak_day_distance],
                         index=['percentage_below_lower', 'percentage_between_thresholds', 'percentage_above_upper', 
                                'variance_factor', 'closest_peak_day_distance', 'largest_peak_day_distance'])

    # Apply the function and add new columns to the DataFrame
    bump_df[['percentage_below_lower', 'percentage_between_thresholds', 'percentage_above_upper', 
             'variance_factor', 'closest_peak_day_distance', 'largest_peak_day_distance']] = bump_df.apply(calculate_metrics, axis=1)
    return bump_df

# Example usage with your DataFrame
event_candidates_df = add_compliance_metrics(event_candidates_df)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
event_candidates_df.columns

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# ### Viusalising distributions of additional columns

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# Setting aesthetics for seaborn
sns.set(style="whitegrid")

def visualize_distributions(data_frame):
    plt.figure(figsize=(18, 12))

    # Histogram and Density Plot for 'percentage_below_lower'
    plt.subplot(2, 3, 1)
    sns.histplot(data_frame['percentage_below_lower'], kde=True, color='blue', bins=30)
    plt.title('Distribution of % Below Lower Threshold')
    plt.xlabel('% of Time Below Lower Threshold')
    plt.ylabel('Frequency')

    # Histogram and Density Plot for 'percentage_between_thresholds'
    plt.subplot(2, 3, 2)
    sns.histplot(data_frame['percentage_between_thresholds'], kde=True, color='purple', bins=30)
    plt.title('Distribution of % Between Thresholds')
    plt.xlabel('% of Time Between Thresholds')
    plt.ylabel('Frequency')

    # Histogram and Density Plot for 'percentage_above_upper'
    plt.subplot(2, 3, 3)
    sns.histplot(data_frame['percentage_above_upper'], kde=True, color='red', bins=30)
    plt.title('Distribution of % Above Upper Threshold')
    plt.xlabel('% of Time Above Upper Threshold')
    plt.ylabel('Frequency')

    # Histogram and Density Plot for 'variance_factor'
    plt.subplot(2, 3, 4)
    sns.histplot(data_frame['variance_factor'], kde=True, color='green', bins=30)
    plt.title('Distribution of Variance Factor')
    plt.xlabel('Min-Max to Threshold Difference Factor')
    plt.ylabel('Frequency')

    # Boxplots for detailed distribution analysis
    plt.subplot(2, 3, 5)
    sns.boxplot(y=data_frame['percentage_below_lower'], color='cyan')
    plt.title('Boxplot of % Below Lower Threshold')
    plt.ylabel('% of Time Below Lower Threshold')

    plt.subplot(2, 3, 6)
    sns.boxplot(y=data_frame['variance_factor'], color='lightgreen')
    plt.title('Boxplot of Variance Factor')
    plt.ylabel('Min-Max to Threshold Difference Factor')

    plt.tight_layout()
    plt.show()

# Assuming 'event_candidates_df' is your DataFrame
visualize_distributions(event_candidates_df)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# ### VISUALISING TIME SERIES WITH BUMPS

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
def visualize_bumps(bump_df, N, sim_column, alpha = 0.05, min_date =  datetime.date(2019,1,1),\
                    max_date= datetime.date(2020,4,24)):
    # Extract the global start and end dates from the DataFrame
    global_start_date = datetime.date(2019,1,1) #bump_df['date'].min()
    global_end_date = datetime.date(2020,4,24) #bump_df['date'].max()

    # Generate a timeline from the global start to end date
    timeline = pd.date_range(start=global_start_date, end=global_end_date)
    
    #
    filtered_df = bump_df[(bump_df["similarity_column"] == sim_column)\
                          &(bump_df["alpha"] == alpha) \
                          &(bump_df["date"] >=min_date)\
                         &(bump_df["date"] <=max_date)]
    
    # Sample N random clusters
    sampled_clusters = filtered_df.sample(n=N, replace=False)

    # Plotting
    fig, axs = plt.subplots(N, 1, figsize=(10, N * 4))

    if N == 1:
        axs = [axs]  # Make sure axs is iterable for a single plot

    for ax, (_, row) in zip(axs, sampled_clusters.iterrows()):
        # Extract smoothed values from the DataFrame
        smoothed_values = row['smoothed_values']

        # Plot original and smoothed values
        ax.plot(timeline, smoothed_values, label=f'LOESS {alpha}', color='red')

        # Highlight the closest bump peak date
        if row['closest_bump_peak_date']:
            ax.axvline(x=row['closest_bump_peak_date'], color='green', linestyle='--', label='Closest Bump Peak')
        
        ax.axvline(x=row['date'], color='orange', linestyle='--', label='Date of topic')
        ax.hlines(y=row["lower_threshold"], xmin = timeline.min(),xmax = timeline.max(),linewidth=1, color='black')
        ax.hlines(y=row["upper_threshold"], xmin = timeline.min(),xmax = timeline.max(),linewidth=1, color='black')

        ax.set_title(f"Topic ID: {row['topic_id']} - Date: {row['date']} - {row['global_tfidf_representation']}")
        ax.set_xlabel('Timeline Date')
        ax.set_ylabel('Smoothed Value')
        ax.legend()

    plt.tight_layout()
    plt.show()

# Example usage
# visualize_bumps(event_candidates_df, N=5, sim_column = "max_sim_global_tfidf_representation_weighted_list", 
#                 alpha = 0.03,
#                 min_date =  datetime.date(2019,1,1),
#                 max_date =  datetime.date(2020,4,24))

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# ### Example of filtering based on time spent in certain inteval, variance_factor and bump-date distance

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# Now you can easily filter and analyze the DataFrame based on the new metrics
max_delta_days = 15
filtered_bump_df = event_candidates_df[(event_candidates_df['percentage_below_lower'] >= 75) \
                                       & (event_candidates_df['variance_factor'] <= 5) \
                                      & (event_candidates_df['closest_peak_day_distance'] <= max_delta_days)] 
print(f"Filtered DataFrame has {len(filtered_bump_df)} rows out of {len(event_candidates_df)} total rows.")

# Optionally, you can inspect the DataFrame to see the distribution or summary statistics
print(filtered_bump_df[['percentage_below_lower', 'variance_factor']].describe())

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
print("Number of bumps: \n", filtered_bump_df.total_bumps.value_counts())
bump_counts = filtered_bump_df.groupby(['similarity_column', 'alpha']).size().reset_index(name='count')
print("\nNumber of event candidates per combination: \n")
bump_counts.sort_values(by="count", ascending = False)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
bump_counts.sort_values(by="count")

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# ### Experimenting with graphs for different filters

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
visualize_bumps(bump_df = filtered_bump_df,
                N=5, 
                sim_column = "max_sim_global_tfidf_representation_weighted_list", 
                alpha = 0.03,
                min_date =  datetime.date(2020,2,1),
                max_date =  datetime.date(2020,4,24))