# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# -*- coding: utf-8 -*-
import dataiku
import pandas as pd, numpy as np
from dataiku import pandasutils as pdu
import matplotlib.pyplot as plt
import seaborn as sns
import random
import datetime
from utils import (
    select_gpu_with_most_free_memory,
    load_experiment_jsons,
    load_experiment_objects,
    get_current_time_and_unix_timestamp,
    get_newest_json,
    get_unique_dictionaries,
    save_combination_list,
)
import ast
from tqdm import tqdm

# Input folders
ed_clusters_with_reps = dataiku.Folder("oiEvOl5p")
ed_clusters_with_reps_path = ed_clusters_with_reps.get_path()

ed_similarity_scores = dataiku.Folder("0JFzvl2d")
ed_similarity_scores_path= ed_similarity_scores.get_path()

ed_similarity_scores_processed = dataiku.Folder("hZfSC2LV")
ed_similarity_scores_processed_path = ed_similarity_scores_processed.get_path()

# Input datasets
weekly_events_merged = dataiku.Dataset("weekly_events_merged")
weekly_events_df = weekly_events_merged.get_dataframe()

key_events_per_period = dataiku.Dataset("key_events_per_period")
key_events_df = key_events_per_period.get_dataframe()

# Fixing lists
weekly_events_df['representative_docs'] = weekly_events_df['representative_docs'].apply(ast.literal_eval)
weekly_events_df['ctfidf_representation'] = weekly_events_df['ctfidf_representation'].apply(ast.literal_eval)
weekly_events_df['keybert_representation'] = weekly_events_df['keybert_representation'].apply(ast.literal_eval)
weekly_events_df['tweet_ids'] = weekly_events_df['tweet_ids'].apply(ast.literal_eval)

# Fixing date columns to get rid of hours, mins, seconds
weekly_events_df['start_date'] = pd.to_datetime(weekly_events_df['start_date']).dt.date
weekly_events_df['end_date'] = pd.to_datetime(weekly_events_df['end_date']).dt.date

# Fixing date columns to get rid of hours, mins, seconds
key_events_df['start_date'] = pd.to_datetime(key_events_df['start_date']).dt.date
key_events_df['end_date'] = pd.to_datetime(key_events_df['end_date']).dt.date

key_events_df.rename(columns={"key_event_id": "topic_id"},inplace = True)
key_events_df = key_events_df.merge(weekly_events_df, on=['topic_id', 'start_date'],suffixes=('', '_y'))
key_events_df.drop(key_events_df.filter(regex='_y$').columns, axis=1, inplace=True)

# Output folder
bump_evaluation = dataiku.Folder("bFPFsUEv")
bump_evaluation_path = bump_evaluation.get_path()

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
print("Number of key ground truth events: ",len(key_events_df))
key_events_df.head(1)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
experiment_jsons = load_experiment_jsons(
                            root_folder_path = ed_similarity_scores_processed_path,
                            dataset_name = "",
                            experiment_details_subfolder = "bump_detection_experiment_details")
# Add filter if necessary
filtered_jsons = experiment_jsons
# filtered_jsons

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# ### LOADING ALL STRUCTURES FOR CURRENT EXPERIMENT

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
for curr_json in filtered_jsons:
    clusters_df  = load_experiment_objects(experiment_jsons = [curr_json],
                            file_path_key = "clustering_save_location")[0]
    
    # Ensure 'clusters_df' has the correct column order
    primary_columns = ['topic_id', 'date', 'cluster_size']
    secondary_columns = [col for col in clusters_df.columns if col not in primary_columns]
    clusters_df = clusters_df[primary_columns + secondary_columns]
    
#     similarity_scores = load_experiment_objects(experiment_jsons = [curr_json],
#                             file_path_key = "similarity_save_location")[0]
    
    bump_df  = load_experiment_objects(experiment_jsons = [curr_json],
                            file_path_key = "bump_save_location")[0]
    
    bump_df = bump_df.merge(clusters_df, on=["topic_id","date"])
    event_candidates_df = bump_df[bump_df["has_bumps"]]

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
event_candidates_df = bump_df[bump_df["has_bumps"]]

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# ### Comparing Bump detection events to GPTs events

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
def calculate_overlap(tweet_ids1, tweet_ids2):
    set1 = set(tweet_ids1)
    set2 = set(tweet_ids2)
    intersection = len(set1.intersection(set2))
    overlap_left = intersection / len(set1) if set1 else 0
    overlap_right = intersection / len(set2) if set2 else 0
    return overlap_left, overlap_right

# Assuming weekly_events_df is loaded and ready
np.random.seed(42)  # For reproducibility
sampled_gpt_events = weekly_events_df.sample(n=200, random_state=42)

# Assuming event_candidates_df has the necessary structure and data
configurations = event_candidates_df[['similarity_column', 'alpha']].drop_duplicates()

# Dictionary to store results DataFrames by configuration
config_results = {}

for _, config in tqdm(configurations.iterrows(), desc="Processing configurations"):
    results = []  # This will store results for the current configuration
    filtered_candidates = event_candidates_df[
        (event_candidates_df['similarity_column'] == config['similarity_column']) &
        (event_candidates_df['alpha'] == config['alpha']) &
        (event_candidates_df['has_bumps'] == True)
    ]

    for _, gpt_event in sampled_gpt_events.iterrows():
        # Temporal filtering: daily events within the same week as the GPT event
        same_week_candidates = filtered_candidates[
            (filtered_candidates['date'] >= gpt_event['start_date']) &
            (filtered_candidates['date'] <= gpt_event['end_date'])
        ]

        for _, candidate in same_week_candidates.iterrows():
            tweet_overlap_left, tweet_overlap_right = calculate_overlap(candidate['tweet_ids'], gpt_event['tweet_ids'])

            # Store the results
            result = {
                'gpt_start_date': gpt_event['start_date'],
                'gpt_end_date': gpt_event['end_date'],
                'gpt_id': gpt_event['topic_id'],
                'gpt_name': gpt_event['event_name'],
                'candidate_date': candidate['date'],
                'candidate_id': candidate['topic_id'],
                'tweet_overlap_left': tweet_overlap_left,
                'tweet_overlap_right': tweet_overlap_right,
                'gpt_keybert': gpt_event['keybert_representation'],
                'candidate_keybert': candidate['keybert_representation'],
            }
            results.append(result)

    # Store each configuration's results in a separate DataFrame within the dictionary
    config_key = (config['similarity_column'], config['alpha'])
    event_mappings = pd.DataFrame(results)
    event_mappings = event_mappings[(event_mappings["tweet_overlap_left"]>=0.5)| \
                                    (event_mappings["tweet_overlap_right"]>=0.5)]
    config_results[config_key] = event_mappings

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
pd.set_option('display.expand_frame_repr', False)
pd.set_option('max_colwidth', -1)
config_results[('max_sim_global_tfidf_representation_weighted_list', 0.03)].sort_values(by =["tweet_overlap_left","tweet_overlap_right"], ascending = False)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# ### Adding columns for filtering - percentage of time above/below thresholds, variance_factor etc.

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
import warnings
warnings.filterwarnings('ignore')

def add_compliance_metrics(bump_df):
    # Define a function to calculate the compliance metrics along with day distances
    def calculate_metrics(row):
        lower_threshold = row['lower_threshold']
        upper_threshold = row['upper_threshold']
        smoothed_values = row['smoothed_values']

        # Calculate the percentage of values below the lower threshold
        below_threshold_count = sum(value <= lower_threshold for value in smoothed_values)
        percentage_below_lower = (below_threshold_count / len(smoothed_values)) * 100

        # Calculate the percentage of values strictly between the lower and upper thresholds
        between_threshold_count = sum(lower_threshold < value < upper_threshold for value in smoothed_values)
        percentage_between_thresholds = (between_threshold_count / len(smoothed_values)) * 100

        # Calculate the percentage of values above the upper threshold
        above_threshold_count = sum(value > upper_threshold for value in smoothed_values)
        percentage_above_upper = (above_threshold_count / len(smoothed_values)) * 100

        # Calculate the min-max variance factor
        max_value = max(smoothed_values)
        min_value = min(smoothed_values)
        variance = max_value - min_value
        threshold_difference = upper_threshold - lower_threshold
        variance_factor = variance / threshold_difference if threshold_difference != 0 else float('inf')  # Prevent division by zero

        # Calculate day distances for closest and largest bump peaks
        closest_peak_day_distance = abs((row['closest_bump_peak_date'] - row['date']).days)
        largest_peak_day_distance = abs((row['largest_bump_peak_date'] - row['date']).days)

        return pd.Series([percentage_below_lower, percentage_between_thresholds, percentage_above_upper, variance_factor, 
                          closest_peak_day_distance, largest_peak_day_distance],
                         index=['percentage_below_lower', 'percentage_between_thresholds', 'percentage_above_upper', 
                                'variance_factor', 'closest_peak_day_distance', 'largest_peak_day_distance'])

    # Apply the function and add new columns to the DataFrame
    bump_df[['percentage_below_lower', 'percentage_between_thresholds', 'percentage_above_upper', 
             'variance_factor', 'closest_peak_day_distance', 'largest_peak_day_distance']] = bump_df.apply(calculate_metrics, axis=1)
    return bump_df

# Example usage with your DataFrame
event_candidates_df = add_compliance_metrics(event_candidates_df)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
event_candidates_df.columns

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# ### Viusalising distributions of additional columns

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# Setting aesthetics for seaborn
sns.set(style="whitegrid")

def visualize_distributions(data_frame):
    plt.figure(figsize=(18, 12))

    # Histogram and Density Plot for 'percentage_below_lower'
    plt.subplot(2, 3, 1)
    sns.histplot(data_frame['percentage_below_lower'], kde=True, color='blue', bins=30)
    plt.title('Distribution of % Below Lower Threshold')
    plt.xlabel('% of Time Below Lower Threshold')
    plt.ylabel('Frequency')

    # Histogram and Density Plot for 'percentage_between_thresholds'
    plt.subplot(2, 3, 2)
    sns.histplot(data_frame['percentage_between_thresholds'], kde=True, color='purple', bins=30)
    plt.title('Distribution of % Between Thresholds')
    plt.xlabel('% of Time Between Thresholds')
    plt.ylabel('Frequency')

    # Histogram and Density Plot for 'percentage_above_upper'
    plt.subplot(2, 3, 3)
    sns.histplot(data_frame['percentage_above_upper'], kde=True, color='red', bins=30)
    plt.title('Distribution of % Above Upper Threshold')
    plt.xlabel('% of Time Above Upper Threshold')
    plt.ylabel('Frequency')

    # Histogram and Density Plot for 'variance_factor'
    plt.subplot(2, 3, 4)
    sns.histplot(data_frame['variance_factor'], kde=True, color='green', bins=30)
    plt.title('Distribution of Variance Factor')
    plt.xlabel('Min-Max to Threshold Difference Factor')
    plt.ylabel('Frequency')

    # Boxplots for detailed distribution analysis
    plt.subplot(2, 3, 5)
    sns.boxplot(y=data_frame['percentage_below_lower'], color='cyan')
    plt.title('Boxplot of % Below Lower Threshold')
    plt.ylabel('% of Time Below Lower Threshold')

    plt.subplot(2, 3, 6)
    sns.boxplot(y=data_frame['variance_factor'], color='lightgreen')
    plt.title('Boxplot of Variance Factor')
    plt.ylabel('Min-Max to Threshold Difference Factor')

    plt.tight_layout()
    plt.show()

# Assuming 'event_candidates_df' is your DataFrame
visualize_distributions(event_candidates_df)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# ### VISUALISING TIME SERIES WITH BUMPS

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
def visualize_bumps(bump_df, N, sim_column, alpha = 0.05, min_date =  datetime.date(2019,1,1),\
                    max_date= datetime.date(2020,4,24)):
    # Extract the global start and end dates from the DataFrame
    global_start_date = datetime.date(2019,1,1) #bump_df['date'].min()
    global_end_date = datetime.date(2020,4,24) #bump_df['date'].max()

    # Generate a timeline from the global start to end date
    timeline = pd.date_range(start=global_start_date, end=global_end_date)
    
    #
    filtered_df = bump_df[(bump_df["similarity_column"] == sim_column)\
                          &(bump_df["alpha"] == alpha) \
                          &(bump_df["date"] >=min_date)\
                         &(bump_df["date"] <=max_date)]
    
    # Sample N random clusters
    sampled_clusters = filtered_df.sample(n=N, replace=False)

    # Plotting
    fig, axs = plt.subplots(N, 1, figsize=(10, N * 4))

    if N == 1:
        axs = [axs]  # Make sure axs is iterable for a single plot

    for ax, (_, row) in zip(axs, sampled_clusters.iterrows()):
        # Extract smoothed values from the DataFrame
        smoothed_values = row['smoothed_values']

        # Plot original and smoothed values
        ax.plot(timeline, smoothed_values, label=f'LOESS {alpha}', color='red')

        # Highlight the closest bump peak date
        if row['closest_bump_peak_date']:
            ax.axvline(x=row['closest_bump_peak_date'], color='green', linestyle='--', label='Closest Bump Peak')
        
        ax.axvline(x=row['date'], color='orange', linestyle='--', label='Date of topic')
        ax.hlines(y=row["lower_threshold"], xmin = timeline.min(),xmax = timeline.max(),linewidth=1, color='black')
        ax.hlines(y=row["upper_threshold"], xmin = timeline.min(),xmax = timeline.max(),linewidth=1, color='black')

        ax.set_title(f"Topic ID: {row['topic_id']} - Date: {row['date']} - {row['global_tfidf_representation']}")
        ax.set_xlabel('Timeline Date')
        ax.set_ylabel('Smoothed Value')
        ax.legend()

    plt.tight_layout()
    plt.show()

# Example usage
# visualize_bumps(event_candidates_df, N=5, sim_column = "max_sim_global_tfidf_representation_weighted_list", 
#                 alpha = 0.03,
#                 min_date =  datetime.date(2019,1,1),
#                 max_date =  datetime.date(2020,4,24))

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# ### Example of filtering based on time spent in certain inteval, variance_factor and bump-date distance

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# Now you can easily filter and analyze the DataFrame based on the new metrics
max_delta_days = 15
filtered_bump_df = event_candidates_df[(event_candidates_df['percentage_below_lower'] >= 75) \
                                       & (event_candidates_df['variance_factor'] <= 5) \
                                      & (event_candidates_df['closest_peak_day_distance'] <= max_delta_days)] 
print(f"Filtered DataFrame has {len(filtered_bump_df)} rows out of {len(event_candidates_df)} total rows.")

# Optionally, you can inspect the DataFrame to see the distribution or summary statistics
print(filtered_bump_df[['percentage_below_lower', 'variance_factor']].describe())

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
print("Number of bumps: \n", filtered_bump_df.total_bumps.value_counts())
bump_counts = filtered_bump_df.groupby(['similarity_column', 'alpha']).size().reset_index(name='count')
print("\nNumber of event candidates per combination: \n")
bump_counts.sort_values(by="count", ascending = False)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
bump_counts.sort_values(by="count")

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# ### Experimenting with graphs for different filters

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
visualize_bumps(bump_df = filtered_bump_df,
                N=20, 
                sim_column = "max_sim_keybert_representation_average_list", 
                alpha = 0.05,
                min_date =  datetime.date(2019,1,1),
                max_date =  datetime.date(2020,4,24))