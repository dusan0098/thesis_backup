# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# -*- coding: utf-8 -*-
import dataiku
import pandas as pd, numpy as np
from dataiku import pandasutils as pdu
import matplotlib.pyplot as plt
import seaborn as sns
import random
import datetime
from sklearn.metrics import precision_score, accuracy_score, f1_score, recall_score
import os
from utils import (
    select_gpu_with_most_free_memory,
    load_experiment_jsons,
    load_experiment_objects,
    get_current_time_and_unix_timestamp,
    get_newest_json,
    get_unique_dictionaries,
    save_combination_list,
)
import ast
from tqdm import tqdm

# Input folders
ed_clusters_with_reps = dataiku.Folder("oiEvOl5p")
ed_clusters_with_reps_path = ed_clusters_with_reps.get_path()

ed_similarity_scores = dataiku.Folder("0JFzvl2d")
ed_similarity_scores_path= ed_similarity_scores.get_path()

ed_similarity_scores_processed = dataiku.Folder("hZfSC2LV")
ed_similarity_scores_processed_path = ed_similarity_scores_processed.get_path()

# Input datasets
weekly_events_merged = dataiku.Dataset("weekly_events_merged")
weekly_events_df = weekly_events_merged.get_dataframe()

key_events_per_period = dataiku.Dataset("key_events_per_period")
key_events_df = key_events_per_period.get_dataframe()

# Fixing lists
weekly_events_df['representative_docs'] = weekly_events_df['representative_docs'].apply(ast.literal_eval)
weekly_events_df['ctfidf_representation'] = weekly_events_df['ctfidf_representation'].apply(ast.literal_eval)
weekly_events_df['keybert_representation'] = weekly_events_df['keybert_representation'].apply(ast.literal_eval)
weekly_events_df['tweet_ids'] = weekly_events_df['tweet_ids'].apply(ast.literal_eval)

# Fixing date columns to get rid of hours, mins, seconds
weekly_events_df['start_date'] = pd.to_datetime(weekly_events_df['start_date']).dt.date
weekly_events_df['end_date'] = pd.to_datetime(weekly_events_df['end_date']).dt.date

# Fixing date columns to get rid of hours, mins, seconds
key_events_df['start_date'] = pd.to_datetime(key_events_df['start_date']).dt.date
key_events_df['end_date'] = pd.to_datetime(key_events_df['end_date']).dt.date

key_events_df.rename(columns={"key_event_id": "topic_id"},inplace = True)
key_events_df = key_events_df.merge(weekly_events_df, on=['topic_id', 'start_date'],suffixes=('', '_y'))
key_events_df.drop(key_events_df.filter(regex='_y$').columns, axis=1, inplace=True)

# Output folder
bump_evaluation = dataiku.Folder("bFPFsUEv")
bump_evaluation_path = bump_evaluation.get_path()

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
print("Number of key ground truth events: ",len(key_events_df))
#key_events_df.head(1)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
experiment_jsons = load_experiment_jsons(
                            root_folder_path = ed_similarity_scores_processed_path,
                            dataset_name = "",
                            experiment_details_subfolder = "bump_detection_experiment_details")
# Add filter if necessary
filtered_jsons = experiment_jsons
# filtered_jsons

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# ### LOADING ALL STRUCTURES FOR CURRENT EXPERIMENT

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
for curr_json in filtered_jsons:
    clusters_df  = load_experiment_objects(experiment_jsons = [curr_json],
                            file_path_key = "clustering_save_location")[0]
    
    # Ensure 'clusters_df' has the correct column order
    primary_columns = ['topic_id', 'date', 'cluster_size']
    secondary_columns = [col for col in clusters_df.columns if col not in primary_columns]
    clusters_df = clusters_df[primary_columns + secondary_columns]
    
#     similarity_scores = load_experiment_objects(experiment_jsons = [curr_json],
#                             file_path_key = "similarity_save_location")[0]
    
    bump_df  = load_experiment_objects(experiment_jsons = [curr_json],
                            file_path_key = "bump_save_location")[0]
    
    original_bump_columns = bump_df.columns
    
    bump_df = bump_df.merge(clusters_df, on=["topic_id","date"])
    event_candidates_df = bump_df[bump_df["has_bumps"]]

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# ### Extracting ground_truth events

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# FOR EACH TOPIC WE COUNT THE NUMBER OF CONFIGS/SIM_COLUMNS FOR WHICH IT HAS CLOSE BUMPS
max_delta_days = 3

# Create a condition to filter rows - has no bump or has bump close by
condition = (bump_df['has_bumps']==False) \
| (abs((bump_df['closest_bump_peak_date'] - bump_df['date']).apply(lambda x: x.days)) <= max_delta_days) 

# Filter the DataFrame based on the condition
close_bumps_df = bump_df[condition]

# Display the filtered DataFrame
close_bumps_df.head(3)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# 1. Counting the number of rows (configs) where has_bumps is True for each cluster (grouped by date and topic_id)
cluster_bump_counts = close_bumps_df.groupby(['date', 'topic_id'])['has_bumps'].sum().reset_index()
cluster_bump_counts.rename(columns={'has_bumps': 'num_bumps'}, inplace=True)

# 2. Counting the number of unique similarity_column values where at least one config has has_bumps as True
# First, create a boolean DataFrame to indicate if there's any bump for each similarity_column within each cluster
bump_presence_per_similarity = close_bumps_df.groupby(['date', 'topic_id', 'similarity_column'])['has_bumps'].any().reset_index()

# Then, count the unique similarity_column values where has_bumps is True for each cluster
similarity_bump_counts = bump_presence_per_similarity.groupby(['date', 'topic_id'])['has_bumps'].sum().reset_index()
similarity_bump_counts.rename(columns={'has_bumps': 'num_similarity_columns_with_bumps'}, inplace=True)

# Ensuring all clusters are included by merging with the original clusters
all_clusters = close_bumps_df[['date', 'topic_id']].drop_duplicates()
similarity_bump_counts = all_clusters.merge(similarity_bump_counts, on=['date', 'topic_id'], how='left').fillna(0)
similarity_bump_counts['num_similarity_columns_with_bumps'] = similarity_bump_counts['num_similarity_columns_with_bumps'].astype(int)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
#cluster_bump_counts.sort_values(by = "num_bumps", ascending = False)
#similarity_bump_counts.sort_values(by = "num_similarity_columns_with_bumps", ascending = False)
sns.set(style="whitegrid")
# Plotting Histogram and Density for Config Counts
plt.figure(figsize=(12, 6))
sns.histplot(cluster_bump_counts['num_bumps'], kde=False, bins=range(0, 34))
plt.title('Histogram and Density for Config Counts')
plt.xlabel('Config number')
plt.ylabel('Frequency')
plt.xticks(range(0, 34, 2))  # Adjust x-axis ticks to show every second number
plt.show()

# Plotting Histogram and Density for Similarity Column Counts
plt.figure(figsize=(12, 6))
sns.histplot(similarity_bump_counts['num_similarity_columns_with_bumps'], kde=False, bins=range(0, 10))
plt.title('Histogram and Density for Similarity Column Counts')
plt.xlabel('Different configs number')
plt.ylabel('Frequency')
plt.xticks(range(0, 10, 1))  # Adjust x-axis ticks to show every number
plt.show()

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# ### Sampling 100 clusters - 25:25:50

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# import os
# # Count the number of clusters in each category
# high_threshold = 3
# medium_threshold = 1

# high_event_clusters = cluster_bump_counts[cluster_bump_counts['num_bumps'] >= high_threshold]
# moderate_event_clusters = cluster_bump_counts[(cluster_bump_counts['num_bumps'] >= medium_threshold)\
#                                               & (cluster_bump_counts['num_bumps'] < high_threshold)]
# low_event_clusters = cluster_bump_counts[cluster_bump_counts['num_bumps'] < medium_threshold]

# # Define the sample sizes for each category
# total_sample_size = 100
# high_event_sample_size = max(25, int((len(high_event_clusters) / len(cluster_bump_counts)) * total_sample_size))
# moderate_event_sample_size = max(25, int((len(moderate_event_clusters) / len(cluster_bump_counts)) * total_sample_size))
# low_event_sample_size = total_sample_size - high_event_sample_size - moderate_event_sample_size

# # Randomly sample clusters within each category
# high_event_sample = high_event_clusters.sample(n=high_event_sample_size, random_state=42)
# moderate_event_sample = moderate_event_clusters.sample(n=moderate_event_sample_size, random_state=42)
# low_event_sample = low_event_clusters.sample(n=low_event_sample_size, random_state=42)

# # Combine the samples
# sampled_clusters = pd.concat([high_event_sample, moderate_event_sample, low_event_sample])

# def assign_category(num_bumps):
#     if num_bumps >= high_threshold:
#         return 'high'
#     elif num_bumps >= medium_threshold:
#         return 'medium'
#     else:
#         return 'low'

# # Add cluster category to sampled_clusters
# sampled_clusters['category'] = sampled_clusters['num_bumps'].apply(assign_category)

# # Merge sampled_clusters with clusters_df to get the full information
# sampled_cluster_info = pd.merge(
#     sampled_clusters,
#     clusters_df,
#     on=['topic_id', 'date'],
#     how='left'
# )

# sampled_cluster_info.head(3)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# sample_pickle_path = os.path.join(bump_evaluation_path, 'sampled_clusters_info_with_categories.pkl')
# sampled_cluster_info.to_pickle(sample_pickle_path)

# Sampled 100 events 
# First filtered those that were never events (category-low), or were events 1-2 times(category-medium),
# 3+ times (category-high) - 50:25:25 samples
# ground_truth_path = os.path.join(bump_evaluation_path, 'sampled_clusters_with_labels.pkl')
# ground_truth_df =  pd.read_pickle(ground_truth_path)
# ground_truth_df.head(3)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# ### Extracting GPT labels for Ground Truth Events

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# import openai
# from openai_utils import handle_rate_limit_error, handle_json_format_error
# # OpenAI configuration
# openai.api_key = "a796cd0d45604c42b9738d7900c11861"
# openai.api_base = "https://topic-representation-long.openai.azure.com/"
# openai.api_type = "azure"
# openai.api_version = '2023-05-15'
# deployment_name = 'ChatGPT-Long'

# openai_config = {
#     "max_tokens": 8000,
#     "temperature": 0.2,
#     "top_p": 0.2
# }

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# system_message = {
#     "role": "system",
#     "content": "You are a helpful assistant. Your primary task is to determine whether the provided cluster of tweets, \
#     extracted from German politicians' Twitter accounts, represents an important event or a regular (everyday) conversation.\
#     The analysis should consider the features and representative documents (tweets) provided for a specific period. \
#     If the content represents an important event, the event should be tied to something exclusive to the period and \
#     not a topic that would be discussed by the politicians on a regular basis. The response should always be in English.\
#     Based on the analysis, please provide a response in a structured JSON format with the following fields: \
#     - 'is_event': Boolean (True if the cluster represents an event, False otherwise) \
#     - 'event_name': A short title for the event (in English, use German words only if necessary) \
#     - 'event_description': A brief explanation of the event in 1-2 sentences (in English, focusing on who, what, where, when, and why) \
#     If the content does not represent an event, set 'is_event' to False and provide default values for 'event_name' and 'event_description'."
# }

# def create_prompt(row):
#     date = row['date']
#     ctfidf_representation = ', '.join(row['ctfidf_representation'])
#     keybert_representation = ', '.join(row['keybert_representation'])
#     representative_docs = '\n'.join(row['repr_docs'])

#     event_examples = """
#     An event should be something significant that had a notable effect, reaction, controversy, or discourse. Here are examples of what should be considered an event:
#     - A controversial law that sparked significant debate or protests
#     - Fights or significant altercations in the parliament
#     - Changes in leadership within the parliament or political parties
#     - Wars or military actions
#     - Natural disasters
#     - Large protests or public demonstrations
#     - Major scandals or political controversies

#     Conversely, a regular conversation should include routine activities or discussions that do not lead to significant reactions or impact. For example:
#     - Routine readings of laws that were expected and had no significant impact
#     - Standard political meetings without notable outcomes
#     - General discussions about ongoing policies without new developments
#     """

#     prompt = f"""
#     I am providing you with the representative features and documents of a cluster extracted from tweets posted by \
#     German politicians. The cluster was created using tweets scraped from their official Twitter accounts, and the\
#     content includes both German and English languages. The day on which these tweets were posted is on {date}.

#     Please analyze the provided features and representative documents to determine whether the content represents\
#     a regular (everyday) conversation or an important event being discussed. An event should be tied to something \
#     exclusive to that day and not a topic that would be discussed by the politicians on a regular basis. The event title \
#     should precisely capture what is happening on that day and why these tweets were posted. \
#     Here are the details:

#     ### Date:
#     - Date: {date}

#     ### Representative Features:
#     - C-TF-IDF Representation: {ctfidf_representation}
#     - KeyBERT Representation: {keybert_representation}

#     ### Representative Documents (Tweets):
#     {representative_docs}

#     {event_examples}

#     Based on this information, is the cluster discussing a regular conversation or an important event? \
#     Please provide your reasoning in the following JSON format:

#     {{
#         "is_event": true or false,
#         "event_name": "string",
#         "event_description": "string"
#     }}.

#     If you determine that the cluster corresponds to an event (is_event is true), please fill out all of the fields with \
#     the following details:
#     - "is_event": true
#     - "event_name": A short title for the event (in English, use German words only if necessary)
#     - "event_description": An explanation of the event with key details. \
#     (in English, focusing on conveying - who, what, where, when, and why. Ensure that the event description explains \
#     the core reason and consequences of the event happening on that day.) If the event is part of a larger ongoing event\
#     you can also provide this context through an additional sentence, after covering the daily event.

#     If you determine that the cluster does not correspond to an event (is_event is false), please fill out the fields with:
#     - "is_event": false
#     - "event_name": ""
#     - "event_description": ""
#     """
#     return prompt

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# def process_row(row):
#     current_prompt = create_prompt(row)
#     messages = [system_message, {"role": "user", "content": current_prompt}]

#     max_attempts = 3  # Maximum attempts to get correct JSON
#     for attempt in range(max_attempts):
#         try:
#             response = openai.ChatCompletion.create(
#                 engine=deployment_name,
#                 messages=messages,
#                 **openai_config
#             )
#             content = response['choices'][0]['message']['content']
#             result = json.loads(content)
#             return result
#         except openai.error.InvalidRequestError as e:
#             if "maximum context length" in str(e):
#                 print(f"Context too long in attempt {attempt + 1}.")
#             else:
#                 print(f"Attempt {attempt + 1} failed. Error: {str(e)}")
#                 break  # Exit the loop for invalid request errors
#         except json.decoder.JSONDecodeError as e:
#             result = handle_json_format_error(
#                 current_messages=messages,
#                 deployment_name=deployment_name,
#                 response=response,
#                 error_message=e,
#                 max_attempts=max_attempts
#             )
#             return result
#         except openai.error.RateLimitError as e:
#             print(f"RateLimitError in attempt {attempt + 1}")
#             if attempt < max_attempts - 1:
#                 error_message = str(e)
#                 handle_rate_limit_error(error_message)
#             else:
#                 print("Maximum retry attempts reached.")
#         except Exception as e:
#             print(f"Error in attempt {attempt + 1}: {str(e)}")
#             if attempt == max_attempts - 1:
#                 handle_rate_limit_error(str(e))
#             else:
#                 time.sleep(5)  # Wait for 5 seconds before retrying
#     return {"is_event": False, "event_name": "", "event_description": ""}

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# import json
# ground_truth_df['gpt_is_event'] = False
# ground_truth_df['gpt_event_name'] = ""
# ground_truth_df['gpt_event_description'] = ""

# for idx, row in tqdm(ground_truth_df.iterrows(), desc="Processing topic clusters"):
#     result = process_row(row)
#     ground_truth_df.at[idx, 'gpt_is_event'] = result.get('is_event', False)
#     if result['is_event']:
#         print("\nEvent name: ", result.get('event_name', "")) 
#         print("\nEvent description: ", result.get('event_description', ""))
#         ground_truth_df.at[idx, 'gpt_event_name'] = result.get('event_name', "")
#         ground_truth_df.at[idx, 'gpt_event_description'] = result.get('event_description', "")
#     else:
#         print('\n not an event, full response: \n', result)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
ground_truth_with_gpt_path = os.path.join(bump_evaluation_path, 'ground_truth_with_gpt_labels.pkl')
#ground_truth_df.to_pickle(ground_truth_with_gpt_path)

ground_truth_df = pd.read_pickle(ground_truth_with_gpt_path)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# Sample 5 rows where label is True
true_samples = ground_truth_df[ground_truth_df['label'] == True].sample(n=5, random_state=42)

# Sample 5 rows where label is False
false_samples = ground_truth_df[ground_truth_df['label'] == False].sample(n=5, random_state=42)

# Combine the samples
sampled_examples = pd.concat([true_samples, false_samples])
pd.set_option('max_colwidth',-1)
sampled_examples[["date","ctfidf_representation", "keybert_representation","global_tfidf_representation","label","label_comment",\
                  "gpt_is_event"]]

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
pd.reset_option('max_colwidth')

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# Extract the true labels and the GPT predictions from ground_truth_df
true_labels = ground_truth_df['label']
gpt_predictions = ground_truth_df['gpt_is_event']

# Calculate precision
precision = precision_score(true_labels, gpt_predictions)

# Calculate accuracy
recall = recall_score(true_labels, gpt_predictions)

# Calculate F1 score
f1 = f1_score(true_labels, gpt_predictions)

# Print the results
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1 Score: {f1:.2f}")

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# ### Extracting results for different configurations - Upper/Lower threshold

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# FUNCTIONS FOR BUMP DETECTION
# Step 1: Label each point as rise or fall
def label_rise_fall(smoothed_values):
    labels = ["-"]
    for i in range(1, len(smoothed_values)):
        if smoothed_values[i] > smoothed_values[i - 1]:
            labels.append("R")
        elif smoothed_values[i] < smoothed_values[i - 1]:
            labels.append("F")
        else:
            labels.append(labels[-1])  # In case of equal values, keep the previous state
    return labels

# Step 2: Identify bumps
def identify_bumps(labels, smoothed_values, timeline):
    bumps = []
    in_rise = False
    bump_start = None
    bump_peak = None
    bump_end = None

    for i in range(1, len(labels)):
        if labels[i] == "R" and not in_rise:
            in_rise = True
            bump_start = i - 1
        elif labels[i] == "F" and in_rise:
            bump_peak = i - 1
            in_rise = False

            # Check for the end of the bump
            if bump_start is not None and bump_peak is not None:
                j = i
                while j < len(labels) and labels[j] == "F":
                    j += 1
                bump_end = j - 1

                # Record the bump
                bumps.append({
                    'start_date': timeline[bump_start].date(),
                    'end_date': timeline[bump_end].date(),
                    'peak_date': timeline[bump_peak].date(),
                    'peak_value': smoothed_values[bump_peak],
                    'start_index': bump_start,
                    'peak_index': bump_peak,
                    'end_index': bump_end
                })

                bump_start = None
                bump_peak = None
                bump_end = None

    return bumps

def detect_bumps_in_series(smoothed_values, timeline, lower_threshold, upper_threshold, min_days_above_upper=7, max_bump_width=45):
    # Execute the steps
    labels = label_rise_fall(smoothed_values)
    bumps = identify_bumps(labels, smoothed_values, timeline)
    
    filtered_bumps = []
    for bump in bumps:
        # Check if start and end are below lower threshold
        if smoothed_values[bump['start_index']] < lower_threshold and smoothed_values[bump['end_index']] < lower_threshold:
            # Check if peak is above upper threshold
            if bump['peak_value'] > upper_threshold:
                # Calculate bump width as the time spent above the lower threshold
                bump_rise_start = next((i for i in range(bump['start_index'], bump['peak_index'] + 1) if smoothed_values[i] > lower_threshold), bump['start_index'])
                bump_fall_end = next((i for i in range(bump['peak_index'] + 1, bump['end_index'] + 1) if smoothed_values[i] < lower_threshold), bump['end_index'])
                
                bump_width = (timeline[bump_fall_end].date() - timeline[bump_rise_start].date()).days
                if bump_width <= max_bump_width:
                    # Check number of days above upper threshold
                    days_above_upper = sum(1 for i in range(bump['start_index'], bump['end_index'] + 1) if smoothed_values[i] > upper_threshold)
                    if days_above_upper >= min_days_above_upper:
                        bump['days_above_lower'] = bump_width
                        bump['days_above_upper'] = days_above_upper
                        # Find the first date above upper threshold
                        bump['first_above_upper_date'] = next((timeline[i].date() for i in range(bump['start_index'], bump['end_index'] + 1) if smoothed_values[i] > upper_threshold), None)
                        filtered_bumps.append(bump)
    return filtered_bumps

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
from joblib import Parallel, delayed
import itertools
from tqdm import tqdm

def calculate_quantiles_from_smoothed(smoothed_values, quantiles):
    all_smoothed_values = np.concatenate(smoothed_values.values)
    lower_threshold = np.percentile(all_smoothed_values, quantiles[0] * 100)
    upper_threshold = np.percentile(all_smoothed_values, quantiles[1] * 100)
    assert lower_threshold < upper_threshold
    return lower_threshold, upper_threshold


# def detect_bumps_in_df(df_wide, timeline_columns, alpha_values, quantiles=(0.3, 0.7), min_days_above_upper=5, max_bump_width=45):
#     bump_records = []
    
#     global_start_date = df_wide['date'].min()
#     global_end_date = df_wide['date'].max()
#     timeline = pd.date_range(start=global_start_date, end=global_end_date)
    
#     start_time = time.time()

#     for col in tqdm(timeline_columns, desc="Processing columns"):
#         for alpha in tqdm(alpha_values, desc="Processing alphas"):
#             current_df = df_wide[(df_wide["similarity_column"]==col) &((df_wide["alpha"]==alpha))]
#             smoothed_values = current_df["smoothed_values"]
#             lower_threshold, upper_threshold = calculate_quantiles_from_smoothed(smoothed_values, quantiles)

#             for idx, row in current_df.iterrows():
#                 smoothed_row_values = smoothed_values[idx]
#                 bumps = detect_bumps_in_series(smoothed_values = smoothed_row_values, 
#                                                timeline = timeline,
#                                                lower_threshold = lower_threshold,
#                                                upper_threshold =upper_threshold,
#                                                min_days_above_upper =min_days_above_upper,
#                                                max_bump_width = max_bump_width)
#                 has_bumps = len(bumps) > 0
#                 closest_bump_peak_date = None
#                 closest_bump_peak_value = None
#                 closest_bump_start_date = None
#                 closest_bump_end_date = None
#                 closest_bump_width = None
#                 closest_bump_time_above_upper = None
#                 closest_bump_time_above_lower = None
                
#                 total_bumps = len(bumps)
#                 largest_bump_peak_date = None
#                 largest_bump_peak_value = None

#                 if has_bumps:                    
#                     closest_bump = min(bumps, key=lambda x: abs(x['peak_date'] - row['date']))
#                     closest_bump_peak_date = closest_bump['peak_date']
#                     closest_bump_peak_value = closest_bump['peak_value']
#                     closest_bump_width = (closest_bump['end_date'] - closest_bump['start_date']).days
#                     closest_bump_time_above_upper = closest_bump['days_above_upper']
#                     closest_bump_time_above_lower = closest_bump['days_above_lower']
#                     closest_bump_start_date = closest_bump['start_date']
#                     closest_bump_end_date = closest_bump['end_date']
                    
#                     largest_bump = max(bumps, key=lambda x: x['peak_value'])
#                     largest_bump_peak_date = largest_bump['peak_date']
#                     largest_bump_peak_value = largest_bump['peak_value']

#                 bump_records.append({
#                     'topic_id': row['topic_id'],
#                     'date': row['date'],
                    
#                     'similarity_column': col,
#                     'alpha': alpha,
                    
#                     'smoothed_values': smoothed_row_values,
#                     'lower_threshold':lower_threshold,
#                     'upper_threshold':upper_threshold,
#                     'has_bumps': has_bumps,
#                     'total_bumps': total_bumps,
                    
#                     'closest_bump_peak_date': closest_bump_peak_date,
#                     'closest_bump_peak_value': closest_bump_peak_value,
#                     'closest_bump_start_date': closest_bump_start_date,
#                     'closest_bump_end_date': closest_bump_end_date,
#                     'closest_bump_width': closest_bump_width,
#                     'closest_bump_time_above_lower': closest_bump_time_above_lower,
#                     'closest_bump_time_above_upper': closest_bump_time_above_upper,
                    
#                     'largest_bump_peak_date': largest_bump_peak_date,
#                     'largest_bump_peak_value': largest_bump_peak_value
#                 })
                
#     end_time = time.time()  
#     time_taken_minutes = (end_time - start_time) / 60 

#     bump_df = pd.DataFrame(bump_records)
#     return bump_df, time_taken_minutes


def detect_bumps_in_series_parallel(row, smoothed_row_values, timeline, lower_threshold, upper_threshold, min_days_above_upper, max_bump_width):
    bumps = detect_bumps_in_series(smoothed_values=smoothed_row_values, 
                                   timeline=timeline,
                                   lower_threshold=lower_threshold,
                                   upper_threshold=upper_threshold,
                                   min_days_above_upper=min_days_above_upper,
                                   max_bump_width=max_bump_width)

    has_bumps = len(bumps) > 0
    closest_bump_peak_date = None
    closest_bump_peak_value = None
    closest_bump_start_date = None
    closest_bump_end_date = None
    closest_bump_width = None
    closest_bump_time_above_upper = None
    closest_bump_time_above_lower = None
    total_bumps = len(bumps)
    largest_bump_peak_date = None
    largest_bump_peak_value = None

    if has_bumps:                    
        closest_bump = min(bumps, key=lambda x: abs(x['peak_date'] - row['date']))
        closest_bump_peak_date = closest_bump['peak_date']
        closest_bump_peak_value = closest_bump['peak_value']
        closest_bump_width = (closest_bump['end_date'] - closest_bump['start_date']).days
        closest_bump_time_above_upper = closest_bump['days_above_upper']
        closest_bump_time_above_lower = closest_bump['days_above_lower']
        closest_bump_start_date = closest_bump['start_date']
        closest_bump_end_date = closest_bump['end_date']
        
        largest_bump = max(bumps, key=lambda x: x['peak_value'])
        largest_bump_peak_date = largest_bump['peak_date']
        largest_bump_peak_value = largest_bump['peak_value']

    return {
        'topic_id': row['topic_id'],
        'date': row['date'],
        
#         'smoothed_values': smoothed_row_values,
        'lower_threshold': lower_threshold,
        'upper_threshold': upper_threshold,
        'has_bumps': has_bumps,
        'total_bumps': total_bumps,
        
        'closest_bump_peak_date': closest_bump_peak_date,
        'closest_bump_peak_value': closest_bump_peak_value,
        'closest_bump_start_date': closest_bump_start_date,
        'closest_bump_end_date': closest_bump_end_date,
        'closest_bump_width': closest_bump_width,
        'closest_bump_time_above_lower': closest_bump_time_above_lower,
        'closest_bump_time_above_upper': closest_bump_time_above_upper,
        
        'largest_bump_peak_date': largest_bump_peak_date,
        'largest_bump_peak_value': largest_bump_peak_value
    }

def detect_bumps_in_df_optimized(df_wide, timeline_columns, alpha_values, quantiles_list, min_days_above_upper_values, max_bump_width_values, n_jobs=-1):
    bump_results_dict = {}
    
    global_start_date = df_wide['date'].min()
    global_end_date = df_wide['date'].max()
    timeline = pd.date_range(start=global_start_date, end=global_end_date)

    start_time = time.time()

    # Precompute filtered DataFrames and quantiles for each combination of similarity_column and alpha
    precomputed_dfs = {}
    quantile_thresholds = {}

    for col in timeline_columns:
        for alpha in alpha_values:
            current_df = df_wide[(df_wide["similarity_column"] == col) & (df_wide["alpha"] == alpha)]
            smoothed_values = current_df["smoothed_values"]
            precomputed_dfs[(col, alpha)] = current_df

            for quantiles in quantiles_list:
                lower_threshold, upper_threshold = calculate_quantiles_from_smoothed(smoothed_values, quantiles)
                quantile_thresholds[(col, alpha, quantiles)] = lower_threshold, upper_threshold

    # Loop over all combinations of the parameters
    for quantiles, min_days_above_upper, max_bump_width in tqdm(itertools.product(quantiles_list, min_days_above_upper_values, max_bump_width_values), desc="Processing all combinations"):
        for col in tqdm(timeline_columns, desc="Processing columns", leave=False):
            for alpha in tqdm(alpha_values, desc="Processing alphas", leave=False):
                current_df = precomputed_dfs[(col, alpha)]
                smoothed_values = current_df["smoothed_values"]
                lower_threshold, upper_threshold = quantile_thresholds[(col, alpha, quantiles)]

                bump_records = Parallel(n_jobs=n_jobs)(delayed(detect_bumps_in_series_parallel)(
                    row, 
                    smoothed_values[idx], 
                    timeline, 
                    lower_threshold, 
                    upper_threshold, 
                    min_days_above_upper, 
                    max_bump_width
                ) for idx, row in current_df.iterrows())

                bump_df = pd.DataFrame(bump_records)
                bump_results_dict[(quantiles, min_days_above_upper, max_bump_width, col, alpha)] = bump_df

    end_time = time.time()  
    time_taken_minutes = (end_time - start_time) / 60 

    return bump_results_dict, time_taken_minutes

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# bump_results_70_30 = bump_df[original_bump_columns]
df_wide = bump_df[["topic_id","date","similarity_column","alpha", "smoothed_values"]]

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# import time
# import itertools

# timeline_columns = df_wide['similarity_column'].unique()
# alpha_values = df_wide['alpha'].unique()

# # Define parameter values
# quantiles_list = [(0.3, 0.7), (0.35, 0.65), (0.4,0.6)] # default (0.3,07)
# min_days_above_upper_values = [1, 3, 5] # default 5
# max_bump_width_values = [10, 20, 45]# default 45



# IF RERUNING FOR EVALUATION MAKE SURE TO FILTER DF_WIDE TO ONLY PROCESS THE GROUND TRUTH ROWS
# bump_results_dict, time_taken = detect_bumps_in_df_optimized(df_wide, 
#                                                              timeline_columns, 
#                                                              alpha_values, 
#                                                              quantiles_list, 
#                                                              min_days_above_upper_values, 
#                                                              max_bump_width_values)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
import pickle
bump_detection_results_path = os.path.join(bump_evaluation_path, 'bump_results_dict.pkl')

# with open(bump_detection_results_path, 'wb') as handle:
#     pickle.dump(bump_results_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
with open(bump_detection_results_path, 'rb') as handle:
    bump_results_dict = pickle.load(handle)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# Initialize an empty dictionary to store the combined DataFrames
combined_bump_results_dict = {}

# Iterate over the bump_results_dict
for key, df in bump_results_dict.items():
    quantiles, min_days_above_upper, max_bump_width, similarity_column, alpha = key
    # Check if the combination of quantiles, min_days_above_upper, max_bump_width is already in the dictionary
    combined_key = (quantiles, min_days_above_upper, max_bump_width)
    if combined_key not in combined_bump_results_dict:
        combined_bump_results_dict[combined_key] = []

    # Append the DataFrame to the list corresponding to the combined key
    df_copy = df.copy()
    df_copy['similarity_column'] = similarity_column
    df_copy['alpha'] = alpha
    combined_bump_results_dict[combined_key].append(df_copy)

# Concatenate the DataFrames within each combined key
for combined_key in combined_bump_results_dict:
    combined_bump_results_dict[combined_key] = pd.concat(combined_bump_results_dict[combined_key], ignore_index=True)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# ### Scores when Bump = Event

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# Initialize lists to store results
config_results = []

# Iterate over each unique configuration
for combined_key, combined_df in combined_bump_results_dict.items():
    quantiles, min_days_above_upper, max_bump_width = combined_key
    for (similarity_column, alpha), config_group in combined_df.groupby(['similarity_column', 'alpha']):
        # Merge combined_bump_df with ground_truth_df on 'topic_id' and 'date'
        merged_df = pd.merge(config_group, ground_truth_df, on=['topic_id', 'date'], how='inner', suffixes=('_bump', '_ground_truth'))

        # Compare 'has_bumps' with 'label' to calculate TP, FP, TN, FN
        y_true = merged_df['label'].astype(int)  # Convert boolean to int (1 for True, 0 for False)
        y_pred = merged_df['has_bumps'].astype(int)  # Convert boolean to int (1 for True, 0 for False)

        precision = precision_score(y_true, y_pred)
        recall = recall_score(y_true, y_pred)
        f1 = f1_score(y_true, y_pred)

        # Append results to the list
        config_results.append({
            'quantiles': quantiles,
            'min_days_above_upper': min_days_above_upper,
            'max_bump_width': max_bump_width,
            'similarity_column': similarity_column,
            'alpha': alpha,
            'precision': precision,
            'recall': recall,
            'f1_score': f1
        })

# Convert results to DataFrame for better visualization
config_results_df = pd.DataFrame(config_results)

config_results_df.sort_values(by=["f1_score", "recall"], ascending=False)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
import matplotlib.pyplot as plt
import seaborn as sns

# Calculate the maximum F1-score for each configuration
config_max_f1 = config_results_df.groupby(['quantiles', 'min_days_above_upper', 'max_bump_width'])['f1_score'].max().reset_index()

# Identify the top 3 configurations with the highest maximum F1-scores
top_3_configs = config_max_f1.nlargest(3, 'f1_score')
print(top_3_configs)

# Create a figure to hold the heatmaps
fig, axes = plt.subplots(1, 3, figsize=(20, 8), sharey=True)

# Iterate over the top 3 configurations and create heatmaps
for i, (index, config) in enumerate(top_3_configs.iterrows()):
    quantiles = config['quantiles']
    min_days_above_upper = config['min_days_above_upper']
    max_bump_width = config['max_bump_width']
    
    # Filter the original results for the current configuration
    config_filter = (
        (config_results_df['quantiles'] == quantiles) & 
        (config_results_df['min_days_above_upper'] == min_days_above_upper) & 
        (config_results_df['max_bump_width'] == max_bump_width)
    )
    config_subset = config_results_df[config_filter]
    
    # Create a pivot table for the current configuration
    f1_pivot = config_subset.pivot(index='similarity_column', columns='alpha', values='f1_score')
    
    # Sort the similarity columns for better visualization
    sorted_similarity_columns = sorted(f1_pivot.index)
    f1_pivot = f1_pivot.reindex(sorted_similarity_columns)
    
    # Create the heatmap
    sns.heatmap(f1_pivot, annot=True, cmap='viridis', linewidths=.5, ax=axes[i])
    axes[i].set_title(f'F1-score Heatmap\nQuantiles: {quantiles}, Min Days Above: {min_days_above_upper}, Max Width: {max_bump_width}')
    axes[i].set_xlabel('Alpha Value')
    axes[i].set_ylabel('Similarity Column')

plt.tight_layout()
plt.show()

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# ### Scores when Bump AND Date of topic within bump = Event

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
import pandas as pd
from sklearn.metrics import precision_score, recall_score, f1_score

# Assuming ground_truth_df is already loaded DataFrames

# Initialize lists to store results
config_results = []

# Iterate over each unique configuration
for combined_key, combined_df in combined_bump_results_dict.items():
    quantiles, min_days_above_upper, max_bump_width = combined_key
    for (similarity_column, alpha), config_group in combined_df.groupby(['similarity_column', 'alpha']):
        # Merge combined_bump_df with ground_truth_df on 'topic_id' and 'date'
        merged_df = pd.merge(config_group, ground_truth_df, on=['topic_id', 'date'], how='inner', suffixes=('_bump', '_ground_truth'))
        
        # Initialize a list to store the refined predictions
        refined_predictions = []
        
        for _, row in merged_df.iterrows():
            if row['has_bumps']:
                # Check if the date falls within the closest bump start and end dates
                if (row['closest_bump_start_date'] <= row['date'] <= row['closest_bump_end_date']):
                    refined_predictions.append(1)  # Predict as event
                else:
                    refined_predictions.append(0)  # Predict as non-event
            else:
                refined_predictions.append(0)  # Predict as non-event if no bumps
        
        # Convert the refined predictions and ground truth labels to integer type
        y_true = merged_df['label'].astype(int)
        y_pred = pd.Series(refined_predictions)
        
        # Calculate precision, recall, and F1-score
        precision = precision_score(y_true, y_pred)
        recall = recall_score(y_true, y_pred)
        f1 = f1_score(y_true, y_pred)
        
        # Append results to the list
        config_results.append({
            'quantiles': quantiles,
            'min_days_above_upper': min_days_above_upper,
            'max_bump_width': max_bump_width,
            'similarity_column': similarity_column,
            'alpha': alpha,
            'precision': precision,
            'recall': recall,
            'f1_score': f1
        })

# Convert results to DataFrame for better visualization
config_results_df = pd.DataFrame(config_results)

config_results_df.sort_values(by = ["f1_score", "recall"], ascending = False)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
import matplotlib.pyplot as plt
import seaborn as sns

# Calculate the maximum F1-score for each configuration
config_max_f1 = config_results_df.groupby(['quantiles', 'min_days_above_upper', 'max_bump_width'])['f1_score'].max().reset_index()

# Identify the top 3 configurations with the highest maximum F1-scores
top_3_configs = config_max_f1.nlargest(3, 'f1_score')
print(top_3_configs)

# Create a figure to hold the heatmaps
fig, axes = plt.subplots(1, 3, figsize=(20, 8), sharey=True)

# Iterate over the top 3 configurations and create heatmaps
for i, (index, config) in enumerate(top_3_configs.iterrows()):
    quantiles = config['quantiles']
    min_days_above_upper = config['min_days_above_upper']
    max_bump_width = config['max_bump_width']
    
    # Filter the original results for the current configuration
    config_filter = (
        (config_results_df['quantiles'] == quantiles) & 
        (config_results_df['min_days_above_upper'] == min_days_above_upper) & 
        (config_results_df['max_bump_width'] == max_bump_width)
    )
    config_subset = config_results_df[config_filter]
    
    # Create a pivot table for the current configuration
    f1_pivot = config_subset.pivot(index='similarity_column', columns='alpha', values='f1_score')
    
    # Sort the similarity columns for better visualization
    sorted_similarity_columns = sorted(f1_pivot.index)
    f1_pivot = f1_pivot.reindex(sorted_similarity_columns)
    
    # Create the heatmap
    sns.heatmap(f1_pivot, annot=True, cmap='viridis', linewidths=.5, ax=axes[i])
    axes[i].set_title(f'F1-score Heatmap\nQuantiles: {quantiles}, Min Days Above: {min_days_above_upper}, Max Width: {max_bump_width}')
    axes[i].set_xlabel('Alpha Value')
    axes[i].set_ylabel('Similarity Column')

plt.tight_layout()
plt.show()

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# ### Bump and Date is within Max_days of Peak = Bump

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
import pandas as pd
from sklearn.metrics import precision_score, recall_score, f1_score
from datetime import timedelta

# Assuming ground_truth_df is already loaded

# Set the max_days variable
max_days = 10  # You can set this to any number of days you want

# Initialize lists to store results
config_results = []

# Iterate over each unique configuration
for combined_key, combined_df in combined_bump_results_dict.items():
    quantiles, min_days_above_upper, max_bump_width = combined_key
    for (similarity_column, alpha), config_group in combined_df.groupby(['similarity_column', 'alpha']):
        # Merge combined_bump_df with ground_truth_df on 'topic_id' and 'date'
        merged_df = pd.merge(config_group, ground_truth_df, on=['topic_id', 'date'], how='inner', suffixes=('_bump', '_ground_truth'))
        
        # Initialize a list to store the refined predictions
        refined_predictions = []
        
        for _, row in merged_df.iterrows():
            if row['has_bumps']:
                # Check if the date is within max_days of the closest bump peak date
                if abs((row['date'] - row['closest_bump_peak_date']).days) <= max_days:
                    refined_predictions.append(1)  # Predict as event
                else:
                    refined_predictions.append(0)  # Predict as non-event
            else:
                refined_predictions.append(0)  # Predict as non-event if no bumps
        
        # Convert the refined predictions and ground truth labels to integer type
        y_true = merged_df['label'].astype(int)
        y_pred = pd.Series(refined_predictions)
        
        # Calculate precision, recall, and F1-score
        precision = precision_score(y_true, y_pred)
        recall = recall_score(y_true, y_pred)
        f1 = f1_score(y_true, y_pred)
        
        # Append results to the list
        config_results.append({
            'quantiles': quantiles,
            'min_days_above_upper': min_days_above_upper,
            'max_bump_width': max_bump_width,
            'similarity_column': similarity_column,
            'alpha': alpha,
            'precision': precision,
            'recall': recall,
            'f1_score': f1
        })

# Convert results to DataFrame for better visualization
config_results_df = pd.DataFrame(config_results)

config_results_df.sort_values(by = ["f1_score"], ascending = False)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
import matplotlib.pyplot as plt
import seaborn as sns

# Calculate the maximum F1-score for each configuration
config_max_f1 = config_results_df.groupby(['quantiles', 'min_days_above_upper', 'max_bump_width'])['f1_score'].max().reset_index()

# Identify the top 3 configurations with the highest maximum F1-scores
top_3_configs = config_max_f1.nlargest(3, 'f1_score')
print(top_3_configs)
# Create a figure to hold the heatmaps
fig, axes = plt.subplots(1, 3, figsize=(20, 8), sharey=True)

# Iterate over the top 3 configurations and create heatmaps
for i, (index, config) in enumerate(top_3_configs.iterrows()):
    quantiles = config['quantiles']
    min_days_above_upper = config['min_days_above_upper']
    max_bump_width = config['max_bump_width']
    
    # Filter the original results for the current configuration
    config_filter = (
        (config_results_df['quantiles'] == quantiles) & 
        (config_results_df['min_days_above_upper'] == min_days_above_upper) & 
        (config_results_df['max_bump_width'] == max_bump_width)
    )
    config_subset = config_results_df[config_filter]
    
    # Create a pivot table for the current configuration
    f1_pivot = config_subset.pivot(index='similarity_column', columns='alpha', values='f1_score')
    
    # Sort the similarity columns for better visualization
    sorted_similarity_columns = sorted(f1_pivot.index)
    f1_pivot = f1_pivot.reindex(sorted_similarity_columns)
    
    # Create the heatmap
    sns.heatmap(f1_pivot, annot=True, cmap='viridis', linewidths=.5, ax=axes[i])
    axes[i].set_title(f'F1-score Heatmap\nQuantiles: {quantiles}, Min Days Above: {min_days_above_upper}, Max Width: {max_bump_width}')
    axes[i].set_xlabel('Alpha Value')
    axes[i].set_ylabel('Similarity Column')

plt.tight_layout()
plt.show()

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# ### Visualisations for best configs

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# # Bump = Event, (0.35, 0.65), 1, 20 - sim column - max_sim_ctfidf_representation_sentence_list, 0.04 
# # Precision, recall, F1 = 0.425532, 0.769231, 0.547945
# best_config = {
#     'quantiles': (0.35, 0.65),
#     'min_days_above_upper': 1,
#     'max_bump_width': 20,
#     'similarity_column':"max_sim_ctfidf_representation_sentence_list",
#     'alpha': 0.04,
# }

# # Bump contains date = Event, 0.4, 0.6, 1 ,20 - sim_column - max_sim_ctfidf_representation_sentence_list, 0.03
# # Precision, recall, F1 = 0.578947, 0.423077, 0.488889
# best_config = {
#     'quantiles': (0.4, 0.6),
#     'min_days_above_upper': 1,
#     'max_bump_width': 20,
#     'similarity_column':"max_sim_ctfidf_representation_sentence_list",
#     'alpha': 0.03,
# }


# Peak close to date = Event, (0.4, 0.6), 1, 20 - sim_column - max_sim_ctfidf_representation_sentence_list, 0.03
# Precision, recall, F1 = 0.600000, 0.461538, 0.521739
# USE THIS TO RECREATE BEST RESULTS 
best_config = {
    'quantiles': (0.4, 0.6),
    'min_days_above_upper': 1,
    'max_bump_width': 20,
    'similarity_column':"max_sim_ctfidf_representation_sentence_list",
    'alpha': 0.03,
}


# # TEMPORARY SOLUTION PUT WHATEVER YOU WANT HERE
# best_config = {
#     'quantiles': (0.4, 0.6),
#     'min_days_above_upper': 5,
#     'max_bump_width': 20,
#     'similarity_column':"max_sim_global_tfidf_representation_weighted_list",
#     'alpha': 0.03,
# }

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
best_config_df = combined_bump_results_dict[(best_config['quantiles'], best_config['min_days_above_upper'], best_config['max_bump_width'])]
best_config_df = best_config_df[(best_config_df['similarity_column']==best_config['similarity_column'])\
                                &(best_config_df['alpha'] ==best_config['alpha'])]

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
bump_df_smooth_values = bump_df[['topic_id', 'date', 'similarity_column','alpha','smoothed_values']]
best_config_df = pd.merge(best_config_df, bump_df_smooth_values, on=['topic_id', 'date', 'similarity_column','alpha'], how='inner', suffixes=('_bump', '_ground_truth'))

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
best_config_df = best_config_df.merge(clusters_df, on=["topic_id","date"])
best_config_df.head(3)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
import pandas as pd

def add_compliance_metrics(bump_df):
    # Define a function to calculate the compliance metrics along with day distances
    def calculate_metrics(row):
        lower_threshold = row['lower_threshold']
        upper_threshold = row['upper_threshold']
        smoothed_values = row['smoothed_values']

        # Calculate the percentage of values below the lower threshold
        below_threshold_count = sum(value <= lower_threshold for value in smoothed_values)
        percentage_below_lower = (below_threshold_count / len(smoothed_values)) * 100

        # Calculate the percentage of values strictly between the lower and upper thresholds
        between_threshold_count = sum(lower_threshold < value < upper_threshold for value in smoothed_values)
        percentage_between_thresholds = (between_threshold_count / len(smoothed_values)) * 100

        # Calculate the percentage of values above the upper threshold
        above_threshold_count = sum(value > upper_threshold for value in smoothed_values)
        percentage_above_upper = (above_threshold_count / len(smoothed_values)) * 100

        # Calculate the min-max variance factor
        max_value = max(smoothed_values)
        min_value = min(smoothed_values)
        variance = max_value - min_value
        threshold_difference = upper_threshold - lower_threshold
        variance_factor = variance / threshold_difference if threshold_difference != 0 else float('inf')  # Prevent division by zero

        # Check if there are bumps and calculate day distances for closest and largest bump peaks
        if row['has_bumps']:
            closest_peak_day_distance = abs((row['closest_bump_peak_date'] - row['date']).days)
            largest_peak_day_distance = abs((row['largest_bump_peak_date'] - row['date']).days)
        else:
            closest_peak_day_distance = None  # Placeholder value when there are no bumps
            largest_peak_day_distance = None  # Placeholder value when there are no bumps

        return pd.Series([percentage_below_lower, percentage_between_thresholds, percentage_above_upper, variance_factor, 
                          closest_peak_day_distance, largest_peak_day_distance],
                         index=['percentage_below_lower', 'percentage_between_thresholds', 'percentage_above_upper', 
                                'variance_factor', 'closest_peak_day_distance', 'largest_peak_day_distance'])

    # Apply the function and add new columns to the DataFrame
    bump_df[['percentage_below_lower', 'percentage_between_thresholds', 'percentage_above_upper', 
             'variance_factor', 'closest_peak_day_distance', 'largest_peak_day_distance']] = bump_df.apply(calculate_metrics, axis=1)
    return bump_df

best_config_df = add_compliance_metrics(best_config_df)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# ### VISUALISING TIME SERIES WITH BUMPS

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
def visualize_bumps(bump_df, N, sim_column = "max_sim_ctfidf_representation_sentence_list", alpha = 0.03,\
                    min_date=datetime.date(2019,1,1), max_date=datetime.date(2020,4,24), with_bump = True, \
                   max_days_from_peak = None, metric_ranges=None):
    # Extract the global start and end dates from the DataFrame
    global_start_date = datetime.date(2019,1,1) #bump_df['date'].min()
    global_end_date = datetime.date(2020,4,24) #bump_df['date'].max()

    # Generate a timeline from the global start to end date
    timeline = pd.date_range(start=global_start_date, end=global_end_date)
    
    #
    filtered_df = bump_df[(bump_df["similarity_column"] == sim_column) 
                          &(bump_df["alpha"] == alpha) 
                          &(bump_df["date"] >=min_date) 
                          &(bump_df["date"] <=max_date)]
    
    if with_bump:
        filtered_df = filtered_df[filtered_df["has_bumps"]==True]
        if max_days_from_peak:
            filtered_df = filtered_df[filtered_df["closest_peak_day_distance"] <= max_days_from_peak]
            
    if metric_ranges:
        for metric, (min_value, max_value) in metric_ranges.items():
            filtered_df = filtered_df[(filtered_df[metric] >= min_value) & (filtered_df[metric] <= max_value)]
    
    # Sample N random clusters
    sampled_clusters = filtered_df.sample(n=N, replace=False)

    # Plotting
    fig, axs = plt.subplots(N, 1, figsize=(10, N * 4))

    if N == 1:
        axs = [axs]  # Make sure axs is iterable for a single plot

    for ax, (_, row) in zip(axs, sampled_clusters.iterrows()):
        # Extract smoothed values from the DataFrame
        smoothed_values = row['smoothed_values']
        representation = row['ctfidf_representation'] #global_tfidf_representation, ctfidf_representation
        date = row['date']
            
        # Plot original and smoothed values
        ax.plot(timeline, smoothed_values, label=f'LOESS {alpha}', color='red')

        # Highlight the closest bump peak date
        if row['closest_bump_peak_date']:
            ax.axvline(x=row['closest_bump_peak_date'], color='green', linestyle='--', label='Closest Bump Peak')
        
        ax.axvline(x=row['date'], color='orange', linestyle='--', label='Date of topic')
        ax.hlines(y=row["lower_threshold"], xmin = timeline.min(),xmax = timeline.max(),linewidth=1, color='black')
        ax.hlines(y=row["upper_threshold"], xmin = timeline.min(),xmax = timeline.max(),linewidth=1, color='black')

        ax.set_title(f"Topic ID: {row['topic_id']} - Date: {date} - {representation[:5]} \n {representation[5:]} ")
        ax.set_xlabel('Timeline Date')
        ax.set_ylabel('Smoothed Value')
        ax.legend()

    plt.tight_layout()
    plt.show()

# Example usage
# visualize_bumps(event_candidates_df, N=5, sim_column = "max_sim_global_tfidf_representation_weighted_list", 
#                 alpha = 0.03,
#                 min_date =  datetime.date(2019,1,1),
#                 max_date =  datetime.date(2020,4,24))

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
ground_truth_df[ground_truth_df["label"]==True].sort_values(by="date")

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
metric_ranges = {
#   'percentage_below_lower': (80, 100),
#    'percentage_between_thresholds': (25, 75),
#    'percentage_above_upper': (0, 50),
#    'variance_factor': (0, 10),
#    'total_bumps':(1,1),
    'topic_id': (0,0),
}

visualize_bumps(best_config_df, 
                N=1, 
                sim_column = "max_sim_ctfidf_representation_sentence_list", #"max_sim_global_tfidf_representation_weighted_list"
                alpha = 0.03,
                min_date =  datetime.date(2020,1,3),
                max_date =  datetime.date(2020,1,3),
                with_bump = True,
                max_days_from_peak = 100,
                metric_ranges = metric_ranges
               )

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# Setting aesthetics for seaborn
sns.set(style="whitegrid")

def visualize_distributions(data_frame):
    plt.figure(figsize=(18, 12))

    # Histogram and Density Plot for 'percentage_below_lower'
    plt.subplot(2, 3, 1)
    sns.histplot(data_frame['percentage_below_lower'], kde=True, color='blue', bins=30)
    plt.title('Distribution of % Below Lower Threshold')
    plt.xlabel('% of Time Below Lower Threshold')
    plt.ylabel('Frequency')

    # Histogram and Density Plot for 'percentage_between_thresholds'
    plt.subplot(2, 3, 2)
    sns.histplot(data_frame['percentage_between_thresholds'], kde=True, color='purple', bins=30)
    plt.title('Distribution of % Between Thresholds')
    plt.xlabel('% of Time Between Thresholds')
    plt.ylabel('Frequency')

    # Histogram and Density Plot for 'percentage_above_upper'
    plt.subplot(2, 3, 3)
    sns.histplot(data_frame['percentage_above_upper'], kde=True, color='red', bins=30)
    plt.title('Distribution of % Above Upper Threshold')
    plt.xlabel('% of Time Above Upper Threshold')
    plt.ylabel('Frequency')

    # Histogram and Density Plot for 'variance_factor'
    plt.subplot(2, 3, 4)
    sns.histplot(data_frame['variance_factor'], kde=True, color='green', bins=30)
    plt.title('Distribution of Variance Factor')
    plt.xlabel('Min-Max to Threshold Difference Factor')
    plt.ylabel('Frequency')

    # Boxplots for detailed distribution analysis
    plt.subplot(2, 3, 5)
    sns.boxplot(y=data_frame['percentage_below_lower'], color='cyan')
    plt.title('Boxplot of % Below Lower Threshold')
    plt.ylabel('% of Time Below Lower Threshold')

    plt.subplot(2, 3, 6)
    sns.boxplot(y=data_frame['variance_factor'], color='lightgreen')
    plt.title('Boxplot of Variance Factor')
    plt.ylabel('Min-Max to Threshold Difference Factor')

    plt.tight_layout()
    plt.show()

# Assuming 'event_candidates_df' is your DataFrame
visualize_distributions(best_config_df[best_config_df["has_bumps"]])

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# ### Example of filtering based on time spent in certain inteval, variance_factor and bump-date distance

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# # Now you can easily filter and analyze the DataFrame based on the new metrics
# max_delta_days = 15
#  #(event_candidates_df['percentage_below_lower'] >= 75) \
# filtered_bump_df = event_candidates_df[(event_candidates_df['percentage_below_lower'] >= 50)
#                                        & (event_candidates_df['variance_factor'] <= 5) 
#                                       & (event_candidates_df['closest_peak_day_distance'] <= max_delta_days)] 
# print(f"Filtered DataFrame has {len(filtered_bump_df)} rows out of {len(event_candidates_df)} total rows.")

# # Optionally, you can inspect the DataFrame to see the distribution or summary statistics
# print(filtered_bump_df[['percentage_below_lower', 'variance_factor']].describe())

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# print("Number of bumps: \n", filtered_bump_df.total_bumps.value_counts())
# bump_counts = filtered_bump_df.groupby(['similarity_column', 'alpha']).size().reset_index(name='count')
# print("\nNumber of event candidates per combination: \n")
# bump_counts.sort_values(by="count", ascending = False)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# bump_counts.sort_values(by="count")

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# ### Experimenting with graphs for different filters

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# ### Adding columns for filtering - percentage of time above/below thresholds, variance_factor etc.

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# import warnings
# warnings.filterwarnings('ignore')

# def add_compliance_metrics(bump_df):
#     # Define a function to calculate the compliance metrics along with day distances
#     def calculate_metrics(row):
#         lower_threshold = row['lower_threshold']
#         upper_threshold = row['upper_threshold']
#         smoothed_values = row['smoothed_values']

#         # Calculate the percentage of values below the lower threshold
#         below_threshold_count = sum(value <= lower_threshold for value in smoothed_values)
#         percentage_below_lower = (below_threshold_count / len(smoothed_values)) * 100

#         # Calculate the percentage of values strictly between the lower and upper thresholds
#         between_threshold_count = sum(lower_threshold < value < upper_threshold for value in smoothed_values)
#         percentage_between_thresholds = (between_threshold_count / len(smoothed_values)) * 100

#         # Calculate the percentage of values above the upper threshold
#         above_threshold_count = sum(value > upper_threshold for value in smoothed_values)
#         percentage_above_upper = (above_threshold_count / len(smoothed_values)) * 100

#         # Calculate the min-max variance factor
#         max_value = max(smoothed_values)
#         min_value = min(smoothed_values)
#         variance = max_value - min_value
#         threshold_difference = upper_threshold - lower_threshold
#         variance_factor = variance / threshold_difference if threshold_difference != 0 else float('inf')  # Prevent division by zero

#         # Calculate day distances for closest and largest bump peaks
#         closest_peak_day_distance = abs((row['closest_bump_peak_date'] - row['date']).days)
#         largest_peak_day_distance = abs((row['largest_bump_peak_date'] - row['date']).days)

#         return pd.Series([percentage_below_lower, percentage_between_thresholds, percentage_above_upper, variance_factor, 
#                           closest_peak_day_distance, largest_peak_day_distance],
#                          index=['percentage_below_lower', 'percentage_between_thresholds', 'percentage_above_upper', 
#                                 'variance_factor', 'closest_peak_day_distance', 'largest_peak_day_distance'])

#     # Apply the function and add new columns to the DataFrame
#     bump_df[['percentage_below_lower', 'percentage_between_thresholds', 'percentage_above_upper', 
#              'variance_factor', 'closest_peak_day_distance', 'largest_peak_day_distance']] = bump_df.apply(calculate_metrics, axis=1)
#     return bump_df

# # Example usage with your DataFrame
# event_candidates_df = add_compliance_metrics(best_config_df)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# ### Viusalising distributions of additional columns

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# # Setting aesthetics for seaborn
# sns.set(style="whitegrid")

# def visualize_distributions(data_frame):
#     plt.figure(figsize=(18, 12))

#     # Histogram and Density Plot for 'percentage_below_lower'
#     plt.subplot(2, 3, 1)
#     sns.histplot(data_frame['percentage_below_lower'], kde=True, color='blue', bins=30)
#     plt.title('Distribution of % Below Lower Threshold')
#     plt.xlabel('% of Time Below Lower Threshold')
#     plt.ylabel('Frequency')

#     # Histogram and Density Plot for 'percentage_between_thresholds'
#     plt.subplot(2, 3, 2)
#     sns.histplot(data_frame['percentage_between_thresholds'], kde=True, color='purple', bins=30)
#     plt.title('Distribution of % Between Thresholds')
#     plt.xlabel('% of Time Between Thresholds')
#     plt.ylabel('Frequency')

#     # Histogram and Density Plot for 'percentage_above_upper'
#     plt.subplot(2, 3, 3)
#     sns.histplot(data_frame['percentage_above_upper'], kde=True, color='red', bins=30)
#     plt.title('Distribution of % Above Upper Threshold')
#     plt.xlabel('% of Time Above Upper Threshold')
#     plt.ylabel('Frequency')

#     # Histogram and Density Plot for 'variance_factor'
#     plt.subplot(2, 3, 4)
#     sns.histplot(data_frame['variance_factor'], kde=True, color='green', bins=30)
#     plt.title('Distribution of Variance Factor')
#     plt.xlabel('Min-Max to Threshold Difference Factor')
#     plt.ylabel('Frequency')

#     # Boxplots for detailed distribution analysis
#     plt.subplot(2, 3, 5)
#     sns.boxplot(y=data_frame['percentage_below_lower'], color='cyan')
#     plt.title('Boxplot of % Below Lower Threshold')
#     plt.ylabel('% of Time Below Lower Threshold')

#     plt.subplot(2, 3, 6)
#     sns.boxplot(y=data_frame['variance_factor'], color='lightgreen')
#     plt.title('Boxplot of Variance Factor')
#     plt.ylabel('Min-Max to Threshold Difference Factor')

#     plt.tight_layout()
#     plt.show()

# # Assuming 'event_candidates_df' is your DataFrame
# visualize_distributions(event_candidates_df)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# visualize_bumps(bump_df = filtered_bump_df,
#                 N=3, 
#                 sim_column = "max_sim_keybert_representation_sentence_list", 
#                 alpha = 0.05,
#                 min_date =  datetime.date(2019,10,1),
#                 max_date =  datetime.date(2019,11,1),
#                )

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# ### Manual labeling Notes

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# notes = [
# (0, False, "SPD possibly handing over passwords through legislation, no clear event or reactions"),
# (1, False, "gratitude towards frontline workers during the COVID-19 crisis, particularly around March 20, 2020"),
# (2, True, "75th anniversary of the liberation of Auschwitz"),
# (3, True, "Iranian retaliatory strikes against US military bases, which occurred shortly after the US drone strike that killed Qasem Soleimani"),
# (4, False, "youtuber Rezo goes viral for criticising all german political parties and claiming they are creating a divide"),
# (5, False, "discussion about transition in transport, only recent info is on lack of train drivers"),
# (6, True, "humanitarian crisis in the Mediterranean, particularly concerning the rescue and distribution of migrants and refugees. immediate rescue needs and alleviate pressure on frontline states like Italy and Malta."),
# (7, True, "initiative proposed by Defense Minister Kramp-Karrenbauer regarding the establishment of an international security zone in northern Syria. It reflects efforts to address the humanitarian crisis and stabilize the conflict-ridden region"),
# (8, True, "debates about reform to childrens benefits and general welfare proposed by Minister Franziska Giffey"),
# (9, False, "debates surrounding proposals for expropriation as a solution to the housing affordability crisis"),
# (10, False, "basic disagreements around government expenses between socialist and liberal parties, no clear event"),
# (11, True, "unveiling and public reaction to Germany's climate package in 2019, adopted 10 days later"),
# (12, False, "parliamentary debate surrounding economic policies, specifically concerning the social market economy, VAT fraud, VAT fraud started being discussed in the media at this period, but no convictions until years later"),
# (13, False, "discussion about tax policy, and issues with tax evasion"),
# (14, False, "discussion about antisemitism within the Muslim community"),
# (15, False, "reflects general ongoing policy discussions and advocacy efforts regarding transportation and climate goals"),
# (16, True, "reaction to terrorist attack in Halle synagogue Germany, mentions antisemitism and right-wind extremists"),
# (17, False, "ongoing societal issues and discussions around job security - no clear event"),
# (18, True, "war in northern Syria, proposals for a UN-led security zone and the responses from various European countries"),
# (19, False, "ongoing discussions and debates related to transportation policies, environmental concerns, and political accountability"),
# (20, False, "controversy surrounding plans for deportations to Syria due to concerns and human rights considerations"),
# (21, False, "primarily focus on the political deadlock and controversies surrounding the expansion of renewable energies, particularly wind and solar power"),
# (22, False, "debates within the German political landscape concerning taxation policies"),
# (23, False, "debates regarding the direction and effectiveness of energy policies in achieving climate targets."),
# (24, False, "discussions and legislative efforts in Germany regarding the rise of right-wing extremism and antisemitism"),
# (25, False, "40 years of the green party "),
# (26, False, "discussions about environmental policies, motivated by Wildfires in Australia, but they are not the focus"),
# (27, True, "70th anniversary of the Council of Europe "),
# (28, True, "reaction to recent murder of Walter Lubcke and the general rise of right-wing extremism"),
# (29, False, "mixed cluster, no clear place/event that it is tied to, mentions of earlier (2018) Marches of Pegida/AfD"),
# (30, True, "Christine Lambrecht's speech condemning right-wing extremism and violence in the Bundestag, addressing issues related to the NSU and specific incidents like the murder of Walter Lbcke."),
# (31, True, "the activities and statements of the AfD faction regarding their visit to Syria and the subsequent release of a documentary"),
# (32, True, "70th anniversary of the Council of Europe "),
# (33, True, "rejection of expansion of CO2 reduction strategies in the Bundestag - as part of broader EU emssions scheme"),
# (34, True, "criticism and reactions to Ursula von der Leyen's appointment as Defense Minister despite previous statements about not seeking such positions, reflecting on credibility and political strategies"),
# (35, False, "general discussion about taxation, digitalization, and social issues like poverty and inequality"),
# (36, False, "mixed cluster - random greetings from Hamburg and issues with providing Ventilators during Covid-19"),
# (37, True, "reaction to classification of wirsindmehr protests as left-wing extremism in Saxony on May 16, 2019. ties in to general issues of neo-nazis in eastern germany"),
# (38, False, "debates surrounding tax policies, particularly the push for tax reductions and economic implications"),
# (39, True, "discussion about the types and availabilty of masks during the Covid-19 pandemic"),
# (40, False, "small cluster, various topics such as parliamentary debates (Bundestag), personal opinions and experiences, calls for action regarding issues like Duogynon, and inquiries about political responses"),
# (41, True, "Day of Persons with Disabilities - discussions about the ongoing challenges faced by people with disabilities, efforts to improve access to education, employment, and healthcare, and the importance of raising awareness and societal support"),
# (42, False, "mixed topic - reactions to SPDs proposed changes to Pension policy, accusations of sexism within the party as one of the female members of SPD was ignored"),
# (43, False, "general campaign statements 3 weeks before EU 2019 elections, AfD calls for more independence from Brussels"),
# (44, False, "impact of wage policies on part-time workers, criticisms of current thresholds, and proposals for dynamic adjustments"),
# (45, False, "debates around the fairness of pension schemes, criticisms of proposed policies, and the impact of pension reforms"),
# (46, True, "International Workers Day (May 1st), themes of solidarity, social justice, and European integration"),
# (47, True, "Day against the Use of Child Soldiers (RedHandDay) - focus on raising awareness, advocating against the use of child soldiers in armed conflicts, and calling for global action to address this humanitarian crisis"),
# (48, False, "introduction and debates surrounding the Grundrente policy in Germany, highlighting discussions on fairness"),
# (49, False, "discussion of participation of women in parlament and how to achieve parity, tied in to 100 years of women's suffrage"),
# (50, False, "range of issues within education, consumer rights and general economic matters being discussed as 2019 ends"),
# (51, False, "mentions of Advent activities (Adventssingen) and reflections on historical writings by Dietrich Bonhoeffer, marking 75 years since their creation."),
# (52, False, "internal discussion about reorganization within the SPD"),
# (53, False, "initiatives on icreasing usage of wind energy in 2020"),
# (54, False, "solving legal ambiguities regarding the Gemeinsame Terrorismusabwehrzentrum (GTAZ) as of November 2019."),
# (55, True, "abortion rights -calls for protests on January 26th at Rosa-Luxemburg-Platz in Berlin against Paragraph 219a, highlighting the importance of sexual self-determination"),
# (56, False, "May 8th anniversary (end of WW2), general reflections "),
# (57, True, "escalation of Covid-19 pandemic, border closure, debates over border policies and closures, economic impacts, and efforts to combat misinformation"),
# (58, False, "range of discussions on human rights, historical events like the peaceful revolution in Berlin, legislative actions needed regarding supply chains, and Germany's stance on international human rights issues"),
# (59, False, "range of discussions - establishment of the German Foundation for Engagement and Volunteerism, Franziska Giffey's role as the Minister for Family Affairs"),
# (60, True, "humanitarian crisis in the Mediterranean Sea, focusing on sea rescue operations"),
# (61, False, "discussions around climate change mitigation strategies focusing on tree planting initiatives, forest management reforms"),
# (62, False, "mixed cluster no clear topic, personal annegdotes"),
# (63, False, "reactions to general statements made during the 2020 Munich Security Council"),
# (64, False, "general discussions on human rights, no clear motivation"),
# (65, False, "discussion series denoted as '3K20', where participants or speakers express their views on standing firm with their opinions and goals"),
# (66, False, "initiatives and debates around traffic safety and policy reform, particularly regarding cycling infrastructure and speed limits"),
# (67, False, "general discussions about Covid-19 pandemic, no clear development"),
# (68, False, "discussions about railway issues in hanburg and local events from the AfD"),
# (69, False, "ongoing discussions and activities related to healthcare policy, emergency services, and digitalization efforts in healthcare"),
# (70, False," remembering past tragedies caused by right-wing extremism, assessing current threats, and advocating for collective action against such ideologies"),
# (71, False, "debates ranging from electoral mechanics to ideological positioning and public perception of political figures"),
# (72, False, "active debates concerning the Identitarian Movement's role in politics and society, with a focus on its classification as a right-wing extremist group"),
# (73, False, "30th anniversary Tiananmen Square Massacre and discussions on relations with china"),
# (74, True, "reactions to Brandenburg and Saxony state elections in 2019"),
# (75, False, "discussions on climate policy, climate goals, and the urgency of addressing climate change"),
# (76, False, "casual conversations"),
# (77, False, "statements about Macrons calls for EU reform ahead of EU elections"),
# (78, False, "criticism about fascist tendencies calling on the recent Thuringen election results (1 month prior)"),
# (79, False, "criticism and debates surrounding the actions and statements of CDU politicians and statements about possible alliances in Thuringen"),
# (80, True, "results of presidential elections in Slovakia, seen as a positive development for liberal and pro-European movements across the continent"),
# (81, False, "analysis of public sentiment and political responses to media reports, polls, and surveys on climate change issues"),
# (82, False, "mixed - human rights advocacy based on 25 years of Rwandan genocide, discussion about rights on International Roma Day"),
# (83, False, "discussion about global vaccine development, and making sure that it is available to all, no clear events"),
# (84, False, "Participation of influencers and politicians like Philipp Amthor in discussions regarding youth involvement in politics and future political rights"),
# (85, False, "mix of human rights issues, domestic policy, and recent diplomatic visits by Frank-Walter Steinmeier"),
# (86, False, "legal and policy debates,  related to constitutional interpretations or legislative proposals, no clear goal or proposal"),
# (87, False, "mix of activities, discussions, personal reflections in Dsseldorf and Berlin, involving local leaders, visitors, and community members."),
# (88, False, "general discussions about migration and human rights, internal debates within the AfD"),
# (89, False, "mix of Discussions and engagements by political figures like Jrgen Klein, focusing on regional agendas and party activities and sports achievemts"),
# (90, True, "reaction to sentence of Ojub Titiev, calls for fair trials and international pressure on Russia to uphold human rights"),
# (91, False, "mix of many local events"),
# (92, False, "mix of internal AfD elections and meeting of ministers"),
# (93, False, "discussions on healthcare policy, patient rights, and decision-making processes"),
# (94, False, "70th anniversary of NATO and discussion about Germany's role within the organisation"),
# (95, False," small mixed cluster no specific context or details"),
# (96, False, "support for EU accession talks involving North Macedonia and Albania from Germany"),
# (97, False, "discussion about possible price increase on CO2 emissions"),
# (98, False, "mix of criticism of Minister Andreas Scheuers plan on road toll/tax as well as changes in Die Linke and SPD"),
# (99, False, "criticism of AfD and Die Linke on their stance for criminalization of sea rescue operations (Seenotrettung).")
# ]

# notes_df = pd.DataFrame(notes, columns=['index', 'label', 'label_comment'])
# sampled_cluster_info['label'] = notes_df['label']
# sampled_cluster_info['label_comment'] = notes_df['label_comment']
# sampled_cluster_info.head(3)
# # key_info = sampled_cluster_info[["date","ctfidf_representation","keybert_representation","global_tfidf_representation","repr_docs"]]
# # index = 2
# # for key,value in key_info.iloc[index].items():
# #     print("Column:",key,", value:", value)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN
# ### Comparing Bump detection events to GPTs events

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# # Ensure uniqueness by dropping duplicates before processing
# gpt_events_df = weekly_events_df
# event_candidates_df = bump_df[bump_df["has_bumps"]]

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# from sentence_transformers import SentenceTransformer
# from sklearn.metrics.pairwise import cosine_similarity
# import pandas as pd
# import datetime
# import numpy as np

# # Load the Sentence Transformer model
# device = select_gpu_with_most_free_memory()
# model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2', device=device)

# def join_and_encode(df, text_columns):
#     """Join terms in specified columns into a string and batch encode them."""
#     for column in tqdm(text_columns, desc = "Processing columns"):
#         df[column + '_joined'] = df[column].apply(lambda terms: ', '.join(terms))
#     unique_texts = pd.concat([df[col + '_joined'] for col in text_columns]).unique()
#     embeddings = model.encode(unique_texts, show_progress_bar=True)
#     # Normalize the embeddings
#     normalized_embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)
#     text_to_embedding = dict(zip(unique_texts, normalized_embeddings))
#     for column in text_columns:
#         df[column + '_embedding'] = df[column + '_joined'].map(text_to_embedding)
#     return df.drop(columns=[col + '_joined' for col in text_columns])

# def prepare_embeddings(df, text_columns, date_column):
#     # Create a copy of the DataFrame to avoid modifying the original
#     df_copy = df[text_columns + ['topic_id', date_column]].copy()
#     # Drop duplicates based on 'topic_id' and the date_column to ensure unique pairs
#     df_copy = df_copy.drop_duplicates(['topic_id', date_column])
#     df_copy.rename(columns={date_column: 'date'}, inplace=True)
#     df_copy.set_index(['topic_id', 'date'], inplace=True)
#     return join_and_encode(df_copy, text_columns)

# # Specify columns for representations
# gpt_text_columns = ['ctfidf_representation', 'keybert_representation']
# candidate_text_columns = ['ctfidf_representation', 'keybert_representation']

# # Prepare embeddings for GPT events and candidates without modifying the original DataFrames
# gpt_embeddings_df = prepare_embeddings(weekly_events_df, gpt_text_columns, 'start_date')
# candidate_embeddings_df = prepare_embeddings(bump_df[bump_df["has_bumps"]], candidate_text_columns, 'date')

# def calculate_cosine_similarity(embedding1, embedding2):
#     """Calculate the cosine similarity between two normalized embeddings."""
#     return np.dot(embedding1, embedding2)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# def calculate_overlap(tweet_ids1, tweet_ids2):
#     set1 = set(tweet_ids1)
#     set2 = set(tweet_ids2)
#     intersection = len(set1.intersection(set2))
#     overlap_left = intersection / len(set1) if set1 else 0
#     overlap_right = intersection / len(set2) if set2 else 0
#     return overlap_left, overlap_right

# # Assuming weekly_events_df is loaded and ready
# np.random.seed(42)  # For reproducibility
# sampled_gpt_events = gpt_events_df#.sample(n=200, random_state=42)

# # Assuming event_candidates_df has the necessary structure and data
# configurations = event_candidates_df[['similarity_column', 'alpha']].drop_duplicates()

# # Dictionary to store results DataFrames by configuration
# config_results = {}

# # Loop over bump detection configs
# for _, config in tqdm(configurations.iterrows(), desc="Processing configurations"):
#     results = []  # This will store results for the current configuration
#     filtered_candidates = event_candidates_df[
#         (event_candidates_df['similarity_column'] == config['similarity_column']) &
#         (event_candidates_df['alpha'] == config['alpha']) &
#         (event_candidates_df['has_bumps'] == True)
#     ]

#     # Preload all candidate embeddings for this config into a dictionary
#     candidate_embeddings_ctfidf = candidate_embeddings_df['ctfidf_representation_embedding'].to_dict()
#     candidate_embeddings_keybert = candidate_embeddings_df['keybert_representation_embedding'].to_dict()

#     for _, gpt_event in sampled_gpt_events.iterrows():
#         gpt_embeddings_ctfidf = gpt_embeddings_df.loc[(gpt_event["topic_id"], gpt_event["start_date"]), 'ctfidf_representation_embedding']  
#         gpt_embeddings_keybert = gpt_embeddings_df.loc[(gpt_event["topic_id"], gpt_event["start_date"]), 'keybert_representation_embedding']
        
#         # Temporal filtering: daily events within the same week as the GPT event
#         same_week_candidates = filtered_candidates[
#             (filtered_candidates['date'] >= gpt_event['start_date']) & (filtered_candidates['date'] <= gpt_event['end_date'])
#         ]

#         for _, candidate in same_week_candidates.iterrows():
#             # Use preloaded embeddings for faster access
#             candidate_key = (candidate["topic_id"], candidate["date"])
#             candidate_embedding_ctfidf = candidate_embeddings_ctfidf[candidate_key]
#             candidate_embedding_keybert = candidate_embeddings_keybert[candidate_key]
            
#             cos_sim_ctfidf = calculate_cosine_similarity(candidate_embedding_ctfidf, gpt_embeddings_ctfidf)
#             cos_sim_keybert = calculate_cosine_similarity(candidate_embedding_keybert, gpt_embeddings_keybert)
            
#             tweet_overlap_left, tweet_overlap_right = calculate_overlap(candidate['tweet_ids'], gpt_event['tweet_ids'])

#             # Store the results
#             result = {
#                 'gpt_start_date': gpt_event['start_date'],
#                 'gpt_end_date': gpt_event['end_date'],
#                 'gpt_id': gpt_event['topic_id'],
#                 'gpt_name': gpt_event['event_name'],
                
#                 'candidate_date': candidate['date'],
#                 'candidate_id': candidate['topic_id'],
                
#                 'tweet_overlap_left': tweet_overlap_left,
#                 'tweet_overlap_right': tweet_overlap_right,
                
#                 'gpt_keybert': gpt_event['keybert_representation'],
#                 'candidate_keybert': candidate['keybert_representation'],
#                 'cos_sim_keybert': cos_sim_keybert,
                
#                 'gpt_ctfidf': gpt_event['ctfidf_representation'],
#                 'candidate_ctfidf': candidate['ctfidf_representation'],
#                 'cos_sim_ctfidf': cos_sim_ctfidf,
#             }
#             results.append(result)

#     # Store each configuration's results in a separate DataFrame within the dictionary
#     config_key = (config['similarity_column'], config['alpha'])
#     event_mappings = pd.DataFrame(results)
#     event_mappings = event_mappings[(event_mappings["tweet_overlap_left"] >= 0.5) | (event_mappings["tweet_overlap_right"] >= 0.5)]
#     config_results[config_key] = event_mappings

# print("Similarity calculations completed.")

# # for _, config in tqdm(configurations.iterrows(), desc="Processing configurations"):
# #     results = []  # This will store results for the current configuration
# #     filtered_candidates = event_candidates_df[
# #         (event_candidates_df['similarity_column'] == config['similarity_column']) &
# #         (event_candidates_df['alpha'] == config['alpha']) &
# #         (event_candidates_df['has_bumps'] == True)
# #     ]

# #     for _, gpt_event in sampled_gpt_events.iterrows():
# #         gpt_embeddings_ctfidf = gpt_embeddings_df.loc[(gpt_event["topic_id"], gpt_event["start_date"]),\
# #                                                       'ctfidf_representation_embedding']  
# #         gpt_embeddings_keybert = gpt_embeddings_df.loc[(gpt_event["topic_id"], gpt_event["start_date"]),\
# #                                                       'keybert_representation_embedding']
        
# #         # Temporal filtering: daily events within the same week as the GPT event
# #         same_week_candidates = filtered_candidates[
# #             (filtered_candidates['date'] >= gpt_event['start_date']) &
# #             (filtered_candidates['date'] <= gpt_event['end_date'])
# #         ]

# #         for _, candidate in same_week_candidates.iterrows():
            
# #             candidate_embeddings_ctfidf = candidate_embeddings_df.loc[(candidate["topic_id"], candidate["date"]),\
# #                                                       'ctfidf_representation_embedding']
# #             candidate_embeddings_keybert = candidate_embeddings_df.loc[(candidate["topic_id"], candidate["date"]),\
# #                                                       'keybert_representation_embedding']
            
            
# #             cos_sim_ctfidf = calculate_cosine_similarity(candidate_embeddings_ctfidf, gpt_embeddings_ctfidf)
# #             cos_sim_keybert = calculate_cosine_similarity(candidate_embeddings_keybert, gpt_embeddings_keybert)
            
# #             tweet_overlap_left, tweet_overlap_right = calculate_overlap(candidate['tweet_ids'], gpt_event['tweet_ids'])

# #             # Store the results
# #             result = {
# #                 'gpt_start_date': gpt_event['start_date'],
# #                 'gpt_end_date': gpt_event['end_date'],
# #                 'gpt_id': gpt_event['topic_id'],
# #                 'gpt_name': gpt_event['event_name'],
                
# #                 'candidate_date': candidate['date'],
# #                 'candidate_id': candidate['topic_id'],
                
# #                 'tweet_overlap_left': tweet_overlap_left,
# #                 'tweet_overlap_right': tweet_overlap_right,
                
# #                 'gpt_keybert': gpt_event['keybert_representation'],
# #                 'candidate_keybert': candidate['keybert_representation'],
# #                 'cos_sim_keybert': cos_sim_keybert,
                
# #                 'gpt_ctfidf': gpt_event['ctfidf_representation'],
# #                 'candidate_ctfidf': candidate['ctfidf_representation'],
# #                 'cos_sim_ctfidf': cos_sim_ctfidf,
# #             }
# #             results.append(result)

# #     # Store each configuration's results in a separate DataFrame within the dictionary
# #     config_key = (config['similarity_column'], config['alpha'])
# #     event_mappings = pd.DataFrame(results)
# #     event_mappings = event_mappings[(event_mappings["tweet_overlap_left"]>=0.5)| \
# #                                     (event_mappings["tweet_overlap_right"]>=0.5)]
# #     config_results[config_key] = event_mappings

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# pd.set_option('display.expand_frame_repr', False)
# pd.set_option('max_colwidth', -1)
# config_results[('max_sim_global_tfidf_representation_weighted_list', 0.03)].sort_values(by =["tweet_overlap_left","tweet_overlap_right"], ascending = False)